{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Implementing a convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a non-vectorized implementation. The convolution works as follows: given an input tensor x with dimensions\n",
    "(batch_size, input_channels, input_width, input_height), an amount of padding, a number of output channels, kernel size, and a stride, we produce an output tensor with dimensions (batch_size, output_channels, output_width, output_height)\n",
    "\n",
    "Question 1: Write a pseudo-code for how you would implement this with a set of nested\n",
    "for loops. The convolution is defined by a set of weights/parameters which we will learn.\n",
    "How do you represent these weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Pseudocode for convolution (non-vectorized)\n",
    "- Weights are represented as a 4D tensor with dimensions (output_channels, input_channels, kernel_width, kernel_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function convolution(input_tensor, weights, bias, padding, stride):\n",
    "    '''\n",
    "    Convolve the input tensor with the given filter weights \n",
    "    input_tensor: 4D tensor with dimensions \n",
    "    (batch_size, input_channels,\n",
    "    input_height, input_width)\n",
    "    weights: 4D tensor (output_channels, \n",
    "    input_channels, \n",
    "    kernel_height, kernel_width)\n",
    "    bias: 1D tensor (output_channels)\n",
    "    padding: int\n",
    "    stride: int\n",
    "    '''\n",
    "    \n",
    "    # Extract input dimensions\n",
    "    batch_size, \\\n",
    "    input_channels, \\\n",
    "    input_height, input_width = dimensions(input_tensor)\n",
    "\n",
    "    # Calculate dimensions of the output tensor\n",
    "    output_height, output_width = compute_output_size((input_height, input_width), \\\n",
    "    (kernel_height, kernel_width), stride, padding)\n",
    "    output_channels = dimensions(weights)[0]\n",
    "\n",
    "    # Initialize the output tensor with zeros\n",
    "    output_tensor = zeros(batch_size, output_channels, output_height, output_width)\n",
    "\n",
    "    # Apply padding to the input tensor\n",
    "    padded_input = apply_padding(input_tensor, padding)\n",
    "\n",
    "    # Loop over every example in the batch\n",
    "    For b in range(batch_size):\n",
    "        # Loop over every output channel\n",
    "        For oc in range(output_channels):\n",
    "            # Loop over the output spatial dimensions\n",
    "            For oh in range(output_height):\n",
    "                For ow in range(output_width):\n",
    "                    # Initialize a variable to store the convolved value\n",
    "                    convolved_value = 0\n",
    "                    \n",
    "                    # Iterate over each input channel\n",
    "                    For ic in range(input_channels):\n",
    "                        # Iterate over the kernel's spatial dimensions\n",
    "                        For kh in range(kernel_height):\n",
    "                            For kw in range(kernel_width):\n",
    "                                # Calculate the indices on the padded input\n",
    "                                i = oh * stride + kh\n",
    "                                j = ow * stride + kw\n",
    "                                \n",
    "                                # Accumulate the weighted sum for the convolution\n",
    "                                convolved_value += padded_input[b, ic, i, j] *\n",
    "                                                   weights[oc, ic, kh, kw]\n",
    "\n",
    "                    # Store the output \n",
    "                    output_tensor[b, oc, oh, ow] = convolved_value \n",
    "\n",
    "    Return output_tensor\n",
    "\n",
    "Function apply_padding(input_tensor, padding):\n",
    "    # Apply zero-padding to the input tensor\n",
    "    # ...\n",
    "\n",
    "Function compute_output_size(input_size, kernel_size, stride, padding):\n",
    "    # Calculate the height and width of the output tensor\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1.Function to compute the output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8, 32, 32)\n",
      "torch.Size([100, 8, 32, 32])\n",
      "The output size is correct.\n"
     ]
    }
   ],
   "source": [
    "def compute_output_size(input_tensor, filters, stride, padding):\n",
    "    \"\"\"\n",
    "    Computes the output size of a convolutional operation for a batch of images.\n",
    "\n",
    "    :param input_tensor: 4D tensor with shape (batch_size, input_channels, input_height, input_width)\n",
    "    :param filters: 4D tensor with shape (output_channels, input_channels, filter_height, filter_width)\n",
    "    :param stride: Integer representing the stride of the convolution.\n",
    "    :param padding: Integer representing the amount of padding added to the input tensor.\n",
    "\n",
    "    :return: Tuple (batch_size, output_channels, output_width, output_height) representing the dimensions of the output tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract batch size and input channels from input tensor, and output channels from filters\n",
    "    batch_size, input_channels = input_tensor.shape[0], input_tensor.shape[1]\n",
    "    output_channels = filters.shape[0]\n",
    "\n",
    "    # Extract the spatial dimensions of the input tensor\n",
    "    input_height, input_width = input_tensor.shape[2], input_tensor.shape[3]\n",
    "\n",
    "    # Extract the spatial dimensions of the filters\n",
    "    filter_height, filter_width = filters.shape[2], filters.shape[3]\n",
    "\n",
    "    # Compute the output spatial dimensions\n",
    "    output_height = ((input_height - filter_height + 2 * padding) // stride) + 1\n",
    "    output_width = ((input_width - filter_width + 2 * padding) // stride) + 1\n",
    "    \n",
    "    # Return the dimensions of the output tensor\n",
    "    return (batch_size, output_channels, output_width, output_height)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming input_tensor and filters are numpy arrays with the correct 4D shapes\n",
    "# and stride and padding are integers.\n",
    "\n",
    "filters = np.random.rand(8, 3, 3, 3) # 8 filters, 3 channels, 3x3 kernel\n",
    "input_tensor = np.random.rand(100, 4, 5, 5) # 100 images, 3 channels, 32x32 input\n",
    "stride = 1\n",
    "padding = 1\n",
    "output_size = compute_output_size(input_tensor, filters, stride, padding)\n",
    "\n",
    "# Actual output size from PyTorch's convolution operation\n",
    "inputs = torch.randn(100, 3, 32, 32)\n",
    "filters = torch.randn(8, 3, 3, 3) \n",
    "actual_output_size = F.conv2d(inputs, filters, padding=padding, stride=stride).shape\n",
    "\n",
    "# Expected output size from our custom compute_output_size function\n",
    "expected_output_size = compute_output_size(inputs, filters, stride, padding)\n",
    "print(expected_output_size)\n",
    "print(actual_output_size)\n",
    "# Convert PyTorch shape to a tuple for comparison\n",
    "actual_output_size = tuple(actual_output_size)\n",
    "\n",
    "# Assert that the sizes match\n",
    "assert actual_output_size == expected_output_size, f\"Expected {expected_output_size}, got {actual_output_size}\"\n",
    "print(\"The output size is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2.vectorized implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract all patches from the input\n",
    "2. Flatten these patches (with all channels) into vectors, arranged as the rows of a\n",
    "matrix X.\n",
    "3. Multiply this matrix by a weight matrix Y = XW.\n",
    "4. Reshape the matrix Y, so that its rows become the pixels of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches (X): torch.Size([100, 27, 1024])\n",
      "Flattened patches (X): torch.Size([102400, 27])\n",
      "Reshaped Kernel (Y): torch.Size([27, 8])\n",
      "Output from Matrix multiplication (Y): torch.Size([102400, 8])\n",
      "Recovered output shape (Y): torch.Size([100, 8, 32, 32])\n",
      "The output size matches the expected size.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def vectorized_convolution(input_tensor, weight, stride=1, padding=1):\n",
    "    # Step 1: Extract all patches from the input tensor\n",
    "    # The size of the weight determines the kernel_size\n",
    "    kernel_size = (weight.shape[2], weight.shape[3])\n",
    "    patches = F.unfold(\n",
    "        input_tensor, \n",
    "        kernel_size=kernel_size, \n",
    "        stride=stride, \n",
    "        padding=padding\n",
    "    )\n",
    "    print(f'Patches (X): {patches.shape}')\n",
    "    \n",
    "    # Step 2: Flatten these patches\n",
    "    # 'unfold' output has shape (batch_size, C * kernel_height * kernel_width, L)\n",
    "    X = patches.transpose(1, 2).reshape(\n",
    "        -1, \n",
    "        weight.shape[1] * kernel_size[0] * kernel_size[1]\n",
    "    )\n",
    "    print(f'Flattened patches (X): {X.shape}')\n",
    "    \n",
    "    # Reshape weights to match the flattened patches\n",
    "    W = weight.reshape(weight.shape[0], -1).t()\n",
    "    print(f'Reshaped Kernel (Y): {W.shape}')\n",
    "    \n",
    "    # Step 3: Multiply by the weight matrix\n",
    "    Y = torch.matmul(X, W)\n",
    "    print(f'Output from Matrix multiplication (Y): {Y.shape}')\n",
    "    \n",
    "    # Step 4: Reshape the matrix Y so that its rows become the pixels of the output tensor\n",
    "    # The output shape will have the following dimensions (batch_size, out_channels, out_height, out_width)\n",
    "    output_height = (input_tensor.shape[2] + 2 * padding - kernel_size[0]) // stride + 1\n",
    "    output_width = (input_tensor.shape[3] + 2 * padding - kernel_size[1]) // stride + 1\n",
    "    output = Y.reshape(\n",
    "        input_tensor.shape[0], \n",
    "        output_height, \n",
    "        output_width, \n",
    "        -1\n",
    "    ).permute(0, 3, 1, 2)\n",
    "    print(f'Recovered output shape (Y): {output.shape}')\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "batch_size = 100\n",
    "in_channels = 3\n",
    "height, width = 32, 32\n",
    "out_channels = 8\n",
    "kernel_height, kernel_width = 3, 3\n",
    "\n",
    "input_tensor = torch.randn(batch_size, in_channels, height, width)\n",
    "weight = torch.randn(out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "output_tensor = vectorized_convolution(input_tensor, weight, stride=1, padding=1)\n",
    "\n",
    "# Check that output size is correct\n",
    "assert output_tensor.shape == (batch_size, out_channels, height, width), \"The output size does not match the expected size.\"\n",
    "\n",
    "print(\"The output size matches the expected size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.1. Unfold pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function unfold(input_tensor, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Extracts sliding local blocks from a batched input tensor.\n",
    "\n",
    "    Parameters:\n",
    "    input_tensor: A 4D tensor of shape (batch_size, channels, height, width).\n",
    "    kernel_size: A tuple (kH, kW) representing the height and width of the kernel.\n",
    "    stride: A tuple (sH, sW) representing the vertical and horizontal strides.\n",
    "    padding: A tuple (pH, pW) representing the padding added to the height and width.\n",
    "\n",
    "    Returns:\n",
    "    A 3D tensor where each patch is flattened into a row vector.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, channels, height, width = get_shape(input_tensor)\n",
    "    kH, kW = kernel_size\n",
    "    sH, sW = stride\n",
    "    pH, pW = padding\n",
    "\n",
    "    # Apply padding to the input tensor\n",
    "    padded_tensor = apply_padding(input_tensor, pH, pW)\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = (height + 2 * pH - kH) // sH + 1\n",
    "    output_width = (width + 2 * pW - kW) // sW + 1\n",
    "\n",
    "    # Initialize an empty list to store the patches\n",
    "    patches = []\n",
    "\n",
    "    # Loop over every example in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Loop over the output spatial dimensions\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                # Calculate the starting and ending indices of the patch\n",
    "                start_h = h * sH\n",
    "                start_w = w * sW\n",
    "                end_h = start_h + kH\n",
    "                end_w = start_w + kW\n",
    "\n",
    "                # Extract the patch and flatten it\n",
    "                patch = flattened_patch(padded_tensor[i, :, start_h:end_h, start_w:end_w])\n",
    "                patches.append(patch)\n",
    "\n",
    "    # Reshape the list of patches into a 3D tensor \n",
    "    # batch_size, input_channels * kernel_height * kernel_width, number_of_patches\n",
    "    output_tensor = reshape_into_3D_tensor(patches, batch_size, output_height, output_width)\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.2.Pytorch Module implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Gradient:\n",
      " tensor([[[[ 3.1861e-05,  2.0721e-04, -7.1808e-06,  ..., -1.1903e-05,\n",
      "           -2.2866e-04, -3.6752e-06],\n",
      "          [ 1.3335e-04, -4.8401e-05, -1.5264e-04,  ...,  3.2371e-04,\n",
      "           -4.0772e-04,  1.5945e-04],\n",
      "          [-3.3814e-05, -4.2344e-05,  2.8530e-04,  ..., -1.0563e-04,\n",
      "           -2.1008e-04,  2.0957e-04],\n",
      "          ...,\n",
      "          [-1.5031e-05, -1.9909e-04, -2.7194e-05,  ...,  3.0652e-04,\n",
      "           -1.8311e-04, -4.9630e-05],\n",
      "          [-5.8489e-05, -2.3196e-04,  9.5496e-05,  ...,  2.4482e-04,\n",
      "           -9.8822e-05,  9.9997e-05],\n",
      "          [ 6.2164e-06, -8.7847e-06,  3.1590e-05,  ...,  7.7269e-05,\n",
      "            8.2641e-06, -1.6220e-04]],\n",
      "\n",
      "         [[-1.3919e-04,  5.0713e-04, -2.9390e-04,  ...,  2.8358e-05,\n",
      "           -7.7159e-05, -3.7528e-05],\n",
      "          [-3.5528e-04,  1.7514e-04, -3.7062e-04,  ...,  7.4205e-04,\n",
      "           -1.5562e-04,  1.9433e-04],\n",
      "          [ 8.6636e-05,  2.9780e-05,  2.2974e-04,  ...,  2.2651e-04,\n",
      "           -3.1301e-04,  4.3710e-04],\n",
      "          ...,\n",
      "          [ 4.5111e-05, -1.2606e-04, -5.8204e-04,  ...,  1.3085e-04,\n",
      "           -2.2340e-04, -7.9417e-05],\n",
      "          [-8.1356e-05,  4.1560e-05, -3.9828e-04,  ...,  2.4510e-04,\n",
      "            4.3289e-04, -1.6105e-04],\n",
      "          [-8.7304e-05,  2.2170e-05, -1.1325e-04,  ..., -1.2336e-04,\n",
      "            2.1709e-04,  6.3308e-05]],\n",
      "\n",
      "         [[-1.0229e-04, -1.9673e-04,  6.1044e-05,  ..., -1.3893e-05,\n",
      "            2.7500e-04,  1.8013e-04],\n",
      "          [-3.9165e-05,  2.9812e-04, -3.6424e-05,  ..., -1.2126e-04,\n",
      "            8.3979e-05, -4.2499e-04],\n",
      "          [ 5.6971e-05, -1.2163e-04, -3.4605e-04,  ..., -8.3181e-05,\n",
      "           -1.9239e-04, -1.2353e-04],\n",
      "          ...,\n",
      "          [-3.3770e-05,  1.2675e-05,  1.6799e-04,  ...,  2.4684e-04,\n",
      "           -4.3108e-05,  1.6425e-04],\n",
      "          [ 8.2978e-05, -1.6986e-04, -1.5628e-04,  ..., -7.8506e-05,\n",
      "           -1.4318e-04, -2.3193e-04],\n",
      "          [-3.1461e-05,  3.3124e-06,  2.2515e-04,  ..., -2.9448e-04,\n",
      "           -3.1453e-05,  7.4157e-05]]],\n",
      "\n",
      "\n",
      "        [[[-3.8250e-05,  1.3972e-04,  1.5128e-04,  ..., -5.6167e-05,\n",
      "            3.9871e-05, -2.9060e-06],\n",
      "          [-2.1982e-05,  1.0931e-05,  2.8206e-05,  ...,  1.1692e-04,\n",
      "            2.0308e-04, -2.2534e-05],\n",
      "          [ 6.7655e-05, -1.3361e-04, -2.1801e-04,  ..., -2.0696e-04,\n",
      "            5.8853e-06, -5.4000e-06],\n",
      "          ...,\n",
      "          [ 2.6981e-04,  4.1867e-05,  7.5203e-05,  ..., -4.9510e-05,\n",
      "           -3.9742e-06,  5.5267e-05],\n",
      "          [ 1.0521e-04,  1.0433e-04,  2.2412e-04,  ...,  1.2694e-04,\n",
      "           -2.9896e-04, -8.0409e-05],\n",
      "          [-4.1358e-06, -1.4093e-04,  7.3147e-05,  ...,  1.9681e-04,\n",
      "            1.5722e-04,  8.7661e-05]],\n",
      "\n",
      "         [[ 2.8575e-05, -1.5561e-04, -1.4309e-04,  ...,  8.5211e-05,\n",
      "           -2.1820e-05, -8.4463e-05],\n",
      "          [ 1.1522e-04,  1.6545e-05, -1.7948e-04,  ..., -4.7171e-04,\n",
      "           -2.3094e-04, -1.8062e-04],\n",
      "          [-2.2514e-05,  6.7998e-05, -2.2771e-05,  ...,  2.5614e-04,\n",
      "            9.4744e-05, -5.7285e-06],\n",
      "          ...,\n",
      "          [ 6.1038e-05, -4.3782e-04,  2.3460e-04,  ...,  3.1733e-04,\n",
      "            1.9079e-04,  5.3292e-05],\n",
      "          [-3.1012e-04, -3.3577e-04,  4.6428e-04,  ...,  2.4681e-04,\n",
      "           -1.2057e-04, -2.7532e-04],\n",
      "          [ 1.5540e-04, -4.6814e-05,  3.3703e-04,  ...,  2.6171e-04,\n",
      "           -2.2347e-04,  1.0959e-05]],\n",
      "\n",
      "         [[ 6.9331e-05,  6.6962e-05,  3.6029e-06,  ...,  6.0338e-05,\n",
      "            1.0491e-04, -3.7206e-05],\n",
      "          [-1.3378e-05, -5.0064e-05, -3.2133e-05,  ..., -1.1123e-04,\n",
      "            3.2477e-04,  1.2491e-04],\n",
      "          [ 5.3079e-05, -2.0349e-04,  4.6472e-05,  ...,  2.2813e-04,\n",
      "            2.3516e-04, -4.2143e-05],\n",
      "          ...,\n",
      "          [ 2.0222e-04,  2.2503e-04, -6.7966e-06,  ..., -5.3968e-04,\n",
      "           -5.4474e-04,  3.1901e-05],\n",
      "          [ 4.5096e-05,  2.2223e-04, -1.9168e-04,  ..., -1.3014e-04,\n",
      "            4.1221e-04,  2.9412e-05],\n",
      "          [ 2.4682e-05, -6.4322e-05, -1.6212e-04,  ...,  6.7533e-05,\n",
      "            1.6392e-04,  3.2024e-05]]],\n",
      "\n",
      "\n",
      "        [[[-4.0790e-05, -2.0509e-04, -1.8061e-04,  ..., -1.7600e-04,\n",
      "            1.1004e-04,  8.6831e-05],\n",
      "          [-8.8192e-05, -8.9369e-05, -2.0012e-04,  ..., -4.2280e-04,\n",
      "           -9.7235e-05, -2.6288e-04],\n",
      "          [-1.3839e-04,  1.2505e-05,  4.9724e-05,  ..., -2.4804e-05,\n",
      "           -8.7977e-05, -1.3942e-04],\n",
      "          ...,\n",
      "          [-1.5998e-04, -2.5584e-04, -6.2494e-05,  ..., -9.8873e-05,\n",
      "           -3.5432e-04,  2.1769e-05],\n",
      "          [ 1.5373e-04, -7.2466e-05,  8.7017e-06,  ..., -2.3750e-04,\n",
      "           -1.7061e-04,  2.2401e-04],\n",
      "          [-1.4639e-04,  2.9837e-05,  2.1487e-04,  ..., -4.2590e-06,\n",
      "            2.8598e-05, -2.9251e-05]],\n",
      "\n",
      "         [[-4.6628e-05, -6.5319e-05, -3.1677e-04,  ...,  6.3656e-05,\n",
      "            2.3055e-05,  1.2000e-04],\n",
      "          [ 2.1861e-04, -3.6601e-04, -7.4407e-05,  ...,  4.7547e-05,\n",
      "            2.5916e-04,  2.8209e-05],\n",
      "          [ 1.9570e-04, -2.4539e-04,  2.3302e-05,  ..., -2.2097e-04,\n",
      "            1.0127e-04, -4.7612e-05],\n",
      "          ...,\n",
      "          [-2.8918e-05,  5.0412e-04, -3.8243e-04,  ..., -5.4794e-04,\n",
      "            2.0139e-05, -3.2773e-04],\n",
      "          [-1.1072e-04,  1.2760e-05, -3.0288e-04,  ..., -3.4902e-04,\n",
      "            4.3852e-05,  2.5801e-04],\n",
      "          [ 2.0321e-05, -7.4040e-05, -4.9124e-06,  ..., -4.6888e-06,\n",
      "           -8.1013e-06,  7.4721e-05]],\n",
      "\n",
      "         [[-1.3806e-04,  2.6017e-04,  7.0393e-05,  ..., -6.1584e-05,\n",
      "           -2.9706e-04, -5.9251e-05],\n",
      "          [ 2.1392e-04,  4.5120e-04, -5.9792e-05,  ..., -1.2252e-05,\n",
      "           -4.1598e-04, -2.8624e-05],\n",
      "          [ 1.5779e-04, -1.2251e-04,  1.1558e-04,  ...,  2.0467e-04,\n",
      "           -2.1958e-04,  1.4092e-04],\n",
      "          ...,\n",
      "          [-1.8865e-04, -7.9933e-06,  1.4522e-04,  ...,  1.6042e-04,\n",
      "            2.5791e-04, -7.3170e-05],\n",
      "          [ 9.7538e-06,  5.7390e-04,  2.0594e-04,  ...,  2.7340e-04,\n",
      "            3.5422e-05, -2.2082e-04],\n",
      "          [ 9.6973e-05, -3.7765e-05, -1.8493e-05,  ..., -5.9215e-05,\n",
      "           -3.1573e-04,  1.1466e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.8534e-05, -1.6802e-04,  1.1054e-04,  ..., -1.6430e-04,\n",
      "            6.7063e-05,  6.7389e-05],\n",
      "          [ 6.3904e-05, -2.9830e-05, -4.8016e-05,  ..., -6.9987e-05,\n",
      "           -1.1938e-04, -1.1368e-04],\n",
      "          [ 1.5828e-04, -5.3565e-05, -2.2489e-05,  ..., -7.9174e-06,\n",
      "           -1.4638e-04,  8.0115e-05],\n",
      "          ...,\n",
      "          [-1.1765e-04, -7.2454e-05, -1.3448e-04,  ..., -9.7129e-05,\n",
      "           -1.8889e-04, -4.2890e-05],\n",
      "          [-2.1551e-04, -5.3922e-05,  1.4613e-04,  ...,  4.5477e-04,\n",
      "            7.8486e-05, -1.1682e-04],\n",
      "          [ 7.9637e-05,  1.6468e-05, -2.0585e-04,  ...,  8.7860e-05,\n",
      "           -1.5913e-05,  4.2610e-05]],\n",
      "\n",
      "         [[ 9.4937e-05, -2.0457e-04,  2.2173e-04,  ..., -1.8942e-05,\n",
      "           -2.1786e-04,  1.9709e-05],\n",
      "          [ 4.6547e-05,  2.6789e-04, -4.6012e-04,  ...,  9.0145e-05,\n",
      "           -3.6133e-04, -1.9537e-04],\n",
      "          [-2.8850e-04, -5.2391e-06,  8.5400e-05,  ..., -9.8202e-05,\n",
      "           -1.8597e-04,  3.0467e-05],\n",
      "          ...,\n",
      "          [-6.2343e-05, -4.7503e-05, -3.5539e-04,  ..., -1.1275e-04,\n",
      "           -7.6120e-05, -7.6377e-05],\n",
      "          [ 8.6495e-05,  4.9347e-05,  7.1306e-05,  ..., -2.5130e-04,\n",
      "            1.3936e-04, -2.1360e-04],\n",
      "          [ 3.1497e-04, -2.0696e-04, -2.3279e-04,  ..., -1.7705e-05,\n",
      "            3.4744e-04,  3.7294e-05]],\n",
      "\n",
      "         [[ 1.6488e-04, -8.2558e-05, -2.8999e-05,  ..., -2.5947e-05,\n",
      "            9.7064e-05, -3.4946e-05],\n",
      "          [-7.8586e-06, -2.2419e-04, -7.6810e-05,  ...,  1.5725e-04,\n",
      "           -4.9696e-05,  2.7174e-06],\n",
      "          [-1.2066e-04,  3.9488e-04,  2.9158e-04,  ...,  3.1055e-04,\n",
      "           -1.5718e-04,  2.8799e-05],\n",
      "          ...,\n",
      "          [-1.1542e-04,  3.0507e-05, -4.7146e-05,  ...,  6.9563e-04,\n",
      "           -2.0926e-04, -2.0100e-04],\n",
      "          [-2.9990e-05,  2.5146e-04, -2.6183e-05,  ...,  1.3380e-04,\n",
      "           -1.0752e-04,  5.8539e-05],\n",
      "          [ 7.3946e-05, -9.4101e-05,  2.4681e-04,  ..., -1.8641e-04,\n",
      "            1.3676e-04, -7.1006e-05]]],\n",
      "\n",
      "\n",
      "        [[[-2.1828e-04, -7.6625e-05, -4.8766e-07,  ..., -7.7924e-05,\n",
      "           -1.9055e-04, -1.1809e-04],\n",
      "          [ 1.2593e-04, -1.0973e-05, -4.0247e-04,  ..., -1.6142e-05,\n",
      "            2.8506e-05,  7.5503e-05],\n",
      "          [ 1.3616e-06,  4.1307e-04, -6.6403e-05,  ...,  1.3132e-04,\n",
      "            3.8057e-04,  1.7136e-04],\n",
      "          ...,\n",
      "          [ 1.6472e-04, -2.9572e-04,  2.8599e-04,  ...,  2.1775e-04,\n",
      "           -1.4455e-04, -7.8827e-05],\n",
      "          [ 1.1542e-05, -1.3408e-05,  1.1866e-04,  ...,  1.6210e-04,\n",
      "           -3.5260e-04, -9.4091e-05],\n",
      "          [ 3.1579e-05, -2.3950e-04, -3.6164e-06,  ...,  1.0502e-06,\n",
      "           -1.2599e-04,  1.7416e-04]],\n",
      "\n",
      "         [[ 6.1602e-05,  2.6086e-04,  2.1995e-05,  ..., -9.9955e-05,\n",
      "           -3.0774e-04, -1.6565e-04],\n",
      "          [ 1.8893e-04,  2.9844e-04, -1.7086e-05,  ..., -6.1196e-05,\n",
      "           -5.4923e-04,  3.4152e-04],\n",
      "          [ 6.5203e-05,  1.5196e-04,  1.8968e-04,  ...,  2.5992e-05,\n",
      "            7.6309e-05,  1.5851e-05],\n",
      "          ...,\n",
      "          [ 2.4843e-04,  7.1704e-05, -3.1920e-04,  ...,  2.1368e-04,\n",
      "            2.6546e-04, -2.5016e-04],\n",
      "          [ 2.7284e-05, -3.3710e-04, -2.1223e-04,  ...,  5.9116e-06,\n",
      "           -2.5143e-04, -2.4951e-05],\n",
      "          [ 1.9295e-04,  1.2683e-04, -1.0719e-04,  ..., -1.9193e-04,\n",
      "           -2.0942e-04,  2.7476e-05]],\n",
      "\n",
      "         [[-1.5325e-04, -1.7493e-04,  9.3369e-06,  ...,  2.7450e-05,\n",
      "            2.8044e-04, -1.5954e-04],\n",
      "          [-3.7265e-05,  1.4052e-04, -3.3621e-04,  ...,  2.4615e-04,\n",
      "            3.8197e-04,  1.2434e-04],\n",
      "          [ 8.7789e-05,  6.6993e-05, -4.8633e-05,  ...,  5.1042e-04,\n",
      "           -3.1236e-05,  2.5974e-04],\n",
      "          ...,\n",
      "          [-1.4627e-04,  4.1422e-04, -1.4197e-04,  ..., -1.2003e-04,\n",
      "           -2.6768e-04, -1.9807e-04],\n",
      "          [ 9.0746e-06,  1.6472e-04,  1.0130e-04,  ...,  2.0738e-04,\n",
      "           -2.9120e-05,  1.5577e-04],\n",
      "          [ 7.5529e-06,  6.8928e-05, -2.5399e-04,  ...,  1.1260e-04,\n",
      "            1.8465e-04, -5.4399e-05]]],\n",
      "\n",
      "\n",
      "        [[[-6.5719e-05, -1.7760e-04,  1.8963e-04,  ...,  1.6603e-04,\n",
      "            2.2660e-04,  1.1266e-04],\n",
      "          [ 2.4718e-05, -3.1720e-05,  9.9953e-05,  ..., -2.7141e-04,\n",
      "            2.5079e-04,  1.2145e-04],\n",
      "          [ 6.1125e-05,  1.0100e-04,  6.3296e-05,  ..., -7.4715e-07,\n",
      "            2.0980e-05, -1.3360e-04],\n",
      "          ...,\n",
      "          [-1.2165e-04,  9.3114e-06,  1.6812e-04,  ..., -9.0751e-05,\n",
      "            8.9653e-05,  4.5540e-05],\n",
      "          [ 8.0479e-05,  1.0908e-04, -2.1262e-04,  ..., -4.8294e-05,\n",
      "           -4.7962e-04,  1.3687e-04],\n",
      "          [ 1.0466e-04, -2.3302e-04,  5.4023e-05,  ...,  8.9948e-05,\n",
      "           -1.5562e-04, -1.0890e-04]],\n",
      "\n",
      "         [[ 1.8347e-04, -1.8679e-04,  4.5214e-04,  ...,  6.3605e-05,\n",
      "            4.9643e-05,  5.1002e-05],\n",
      "          [ 1.1131e-04, -2.6907e-04,  1.2484e-04,  ...,  3.4277e-04,\n",
      "           -2.8101e-04,  2.4599e-04],\n",
      "          [-7.3611e-05,  5.4949e-05,  2.4197e-04,  ...,  2.0966e-04,\n",
      "            8.2787e-05,  1.3959e-05],\n",
      "          ...,\n",
      "          [ 5.9432e-05, -6.9454e-05,  2.1673e-04,  ..., -2.5271e-04,\n",
      "           -1.0885e-04,  1.3479e-04],\n",
      "          [-5.3206e-05,  2.0140e-04,  1.5558e-05,  ..., -5.5959e-05,\n",
      "           -3.8542e-05, -3.8706e-05],\n",
      "          [-1.2969e-04, -6.2341e-05, -5.1398e-04,  ...,  3.4903e-04,\n",
      "           -1.6259e-04,  3.2917e-06]],\n",
      "\n",
      "         [[-1.0641e-05, -1.4676e-04, -2.0692e-04,  ..., -1.5298e-04,\n",
      "            3.1134e-04, -1.0123e-04],\n",
      "          [-5.1703e-05, -1.0987e-04,  1.0655e-04,  ..., -2.4208e-04,\n",
      "           -2.9207e-04, -1.7008e-05],\n",
      "          [-1.7639e-04,  1.1412e-04, -8.3433e-05,  ...,  1.6084e-04,\n",
      "            8.9761e-05,  1.3435e-04],\n",
      "          ...,\n",
      "          [ 2.4423e-05, -8.6345e-05,  2.1066e-04,  ..., -2.5391e-04,\n",
      "           -1.6253e-04,  4.5030e-05],\n",
      "          [-8.4974e-05, -2.0605e-04,  3.7433e-05,  ..., -1.0484e-04,\n",
      "            1.6036e-04, -1.5078e-04],\n",
      "          [ 9.2966e-05,  2.0902e-05, -1.1077e-04,  ...,  2.0423e-05,\n",
      "            1.5286e-04, -5.6642e-05]]]])\n",
      "Kernel Gradient:\n",
      " tensor([[-3.0560e-01,  3.3401e-01, -3.2291e-01,  1.7094e-01,  9.8930e-02,\n",
      "         -4.1779e-01, -5.4520e-02,  3.0900e-01, -1.8815e-01, -3.0176e-02,\n",
      "         -2.5985e-01, -4.5452e-02, -6.5102e-01, -7.7711e-02, -5.5527e-01,\n",
      "          7.7792e-02, -1.8950e-01, -1.0493e-01,  2.0425e-02, -5.5371e-01,\n",
      "         -4.8822e-04,  1.8022e-01,  4.7341e-01,  3.9872e-01,  2.2011e-01,\n",
      "         -5.7137e-02,  1.8652e-01],\n",
      "        [ 1.5123e-01, -5.7959e-02, -7.5109e-02,  3.8056e-02, -3.6721e-03,\n",
      "          4.1870e-01, -8.6738e-02, -1.5322e-01,  1.4037e-01,  1.8473e-01,\n",
      "         -4.8488e-01,  3.5901e-01,  3.6286e-01, -1.0262e-01,  3.5544e-01,\n",
      "          2.5126e-01, -2.3651e-02,  3.5459e-01,  4.0056e-01, -9.5377e-02,\n",
      "         -2.9483e-01,  1.3940e-01,  8.8514e-02,  1.3345e-01, -1.6689e-01,\n",
      "          1.1308e-01,  1.9004e-01],\n",
      "        [ 1.3784e-01,  1.8632e-01,  2.8375e-01,  2.6941e-01,  1.0050e-01,\n",
      "          2.7485e-01, -1.7833e-01, -8.0202e-02,  9.3151e-02, -7.5192e-02,\n",
      "          3.2041e-01,  1.1690e-01,  2.2678e-01, -2.3440e-01, -3.3615e-01,\n",
      "         -3.2010e-01,  2.7987e-02,  2.0715e-01, -1.0462e-01, -1.5775e-03,\n",
      "         -1.6610e-02,  1.7332e-01, -2.3072e-01, -3.2226e-01, -1.5268e-02,\n",
      "          1.7664e-01,  7.0813e-03],\n",
      "        [ 3.4861e-02, -2.4621e-01,  2.3073e-01,  1.7676e-01, -2.2275e-01,\n",
      "         -4.5353e-01,  8.7863e-02, -5.6957e-02,  1.7562e-01, -3.9156e-02,\n",
      "         -7.0752e-02,  2.0463e-01,  7.8523e-02,  4.4298e-01,  5.7278e-02,\n",
      "         -7.2762e-01, -5.6667e-02, -5.8810e-02, -9.8466e-02, -3.2892e-01,\n",
      "         -4.5453e-01, -7.5108e-02,  1.6535e-02, -3.3564e-01, -4.2384e-02,\n",
      "          1.5413e-01,  3.7494e-01],\n",
      "        [-9.8659e-02, -7.8977e-03,  9.9877e-02, -4.6001e-02, -2.9309e-01,\n",
      "         -5.1066e-01,  2.5152e-02, -2.2168e-01,  1.3471e-01,  3.1990e-01,\n",
      "         -2.4826e-01, -9.5910e-02, -1.8816e-01,  1.6285e-01, -3.8566e-01,\n",
      "          3.3431e-02,  3.4291e-01,  7.6724e-02,  1.7796e-01, -6.0748e-01,\n",
      "          1.6610e-01, -4.2383e-02, -1.9666e-01,  5.7991e-01, -1.8698e-01,\n",
      "          4.5773e-01, -1.8043e-01],\n",
      "        [ 4.4228e-02, -5.3215e-02,  3.1654e-01,  2.5598e-01, -1.1226e-01,\n",
      "          2.3445e-02,  2.5271e-01,  3.1873e-02, -3.3597e-01,  1.2498e-01,\n",
      "         -4.2220e-01,  7.1865e-02, -7.2276e-02, -1.3194e-01, -4.2581e-04,\n",
      "          7.4080e-02,  1.7781e-01,  6.7493e-02,  5.5945e-02,  2.1331e-01,\n",
      "          1.4769e-01,  1.3822e-01,  3.9336e-01, -2.7767e-01,  1.3317e-01,\n",
      "          2.6486e-01, -3.5895e-01],\n",
      "        [ 3.5471e-01,  1.6623e-01, -8.1500e-02,  4.2818e-01,  1.5875e-01,\n",
      "          2.4621e-01, -5.2366e-01,  6.1056e-02,  3.8925e-01, -1.2944e-01,\n",
      "          1.9689e-01, -3.0981e-01, -4.4051e-01, -1.4489e-01,  2.6315e-02,\n",
      "         -1.1446e-01, -3.5531e-01,  3.7875e-01,  3.3311e-01, -8.4046e-02,\n",
      "          2.2523e-02, -3.9604e-02,  6.8102e-02, -1.5540e-01, -2.8594e-01,\n",
      "          8.1557e-03,  9.0930e-02],\n",
      "        [ 3.0019e-02, -2.2737e-01,  2.1667e-01,  3.1520e-01,  4.2682e-01,\n",
      "          2.1312e-01,  1.0477e-01, -1.9544e-01,  9.8344e-02, -2.1334e-01,\n",
      "          5.4756e-02,  4.1919e-01, -1.3168e-01,  5.5237e-01, -1.0845e-01,\n",
      "         -1.3600e-01,  2.6137e-01, -2.5434e-01,  4.1004e-02, -2.9909e-01,\n",
      "          2.4756e-01, -2.8625e-01, -3.1303e-01,  4.1306e-01, -1.6723e-01,\n",
      "         -3.0220e-02, -1.9635e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.kernel = nn.Parameter(torch.randn(out_channels, in_channels * kernel_size[0] * kernel_size[1]))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        b, c, h, w = input_batch.size()\n",
    "\n",
    "        # Get all input patches\n",
    "        patches = F.unfold(input_batch, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        # print(patches.shape)\n",
    "\n",
    "        # Flatten patches into row vectors and treat both b and p as batch dimensions\n",
    "        # Reshape patches to a (b*p, k) tensor for batched matrix multiplication\n",
    "        X = patches.transpose(1, 2).reshape(-1, c * self.kernel_size[0] * self.kernel_size[1])\n",
    "        #print(X.shape)\n",
    "        \n",
    "        # Matrix multiplication with the kernel matrix\n",
    "        Y = X.matmul(self.kernel.t())\n",
    "        #print(Y.shape)\n",
    "\n",
    "        # Calculate expected output dimensions\n",
    "        expected_h, expected_w = ((h + 2 * self.padding - self.kernel_size[0]) // self.stride + 1,\n",
    "                                  (w + 2 * self.padding - self.kernel_size[1]) // self.stride + 1)\n",
    "\n",
    "        # Reshape Y back from (b*p, k) to ouput tensor dimensions\n",
    "        output = Y.reshape(b, expected_h, expected_w, self.out_channels).permute(0, 3, 1, 2)\n",
    "        #print(output.shape)\n",
    "\n",
    "        # Assert the output dimensions\n",
    "        assert output.shape[1:] == (self.out_channels, expected_h, expected_w), \"Output dimensions do not match expected dimensions\"\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "in_channels = 3\n",
    "out_channels = 8\n",
    "conv = Conv2D(in_channels, out_channels)\n",
    "input_batch = torch.randn(100, in_channels, 32, 32, requires_grad=True)\n",
    "output_batch = conv(input_batch)\n",
    "\n",
    "# Implement backward pass\n",
    "loss_function = nn.MSELoss()\n",
    "target_batch = torch.randn(100, out_channels, 32, 32)  # Dummy target output for loss calculation\n",
    "loss = loss_function(output_batch, target_batch)\n",
    "\n",
    "# Backward pass with Pytorch module\n",
    "loss.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(\"Input Gradient:\\n\", input_batch.grad)\n",
    "print(\"Kernel Gradient:\\n\", conv.kernel.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.3. Pytorch Function implementation (see Q4 for backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102400, 8])\n",
      "torch.Size([100, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class Conv2DFunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        \n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        \n",
    "        b, c, h, w = input_batch.size()\n",
    "        out_channels, in_channels, kh, kw = kernel.shape\n",
    "        \n",
    "        # Get all input patches\n",
    "        patches = F.unfold(input_batch, (kh, kw), padding= padding, stride=stride)\n",
    "        \n",
    "        # Flatten patches into row vectors\n",
    "        X = patches.transpose(1, 2).reshape(-1, c * kh * kw)\n",
    "        \n",
    "        # Matrix multiplication with the kernel matrix\n",
    "        Y = X.matmul(kernel.view(kernel.size(0), -1).t())\n",
    "        print(Y.shape)\n",
    "        \n",
    "        # Calculate expected output dimensions\n",
    "        expected_h, expected_w = ((h + 2 * padding - kh) // stride + 1,\n",
    "                                  (w + 2 * padding - kw) // stride + 1)\n",
    "        \n",
    "        # Reshape Y to match the expected output dimensions\n",
    "        output = Y.reshape(b, -1, expected_h, expected_w).permute(0, 1, 2, 3)\n",
    "        \n",
    "        # Assert that the output dimensions match the expected dimensions\n",
    "        assert output.shape[2] == expected_h and output.shape[3] == expected_w, \"Output dimensions do not match expected dimensions\"\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# Define the number of input and output channels\n",
    "input_batch = torch.randn(100, 3, 32, 32)\n",
    "out_channels = 8\n",
    "kernel = torch.randn(out_channels, in_channels, 3, 3)\n",
    "output = Conv2DFunction.apply(input_batch, kernel)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q.4 Gradient of the loss w.r.t. weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 25])\n",
      "torch.Size([100, 27, 25])\n",
      "torch.Size([100, 2, 27])\n",
      "Kernel gradient torch.Size([2, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions and create random tensors for input_batch and grad_output\n",
    "n, c_in, h, w = 100, 3, 5, 5  # Example dimensions\n",
    "c_out, k_h, k_w = 2, 3, 3  # Kernel dimensions\n",
    "\n",
    "# Create random Tensors for input_batch, grad_output, and kernel\n",
    "input_batch = torch.randn(n, c_in, h, w, requires_grad=True)\n",
    "grad_output = torch.randn(n, c_out, h, w)  # Assuming h_out and w_out are same as h and w for simplicity\n",
    "kernel = torch.randn(c_out, c_in, k_h, k_w, requires_grad=True)\n",
    "\n",
    "# Perform batch matrix multiplication to get the gradient with respect to the kernel weights.\n",
    "# Reshape grad_output (n, c_out, h_out*w_out)\n",
    "grad_output_reshaped = grad_output.view(n, c_out, -1)\n",
    "print(grad_output_reshaped.shape)\n",
    "\n",
    "# Reshape input_batch to (n, h*w, c_in) to perform batch matrix multiplication with grad_output_reshaped\n",
    "input_batch_unfolded = F.unfold(input_batch, (k_h, k_w), stride=1, padding=1)\n",
    "print(input_batch_unfolded.shape)\n",
    "\n",
    "# Perform the batch matrix multiplication\n",
    "grad_w_mul = torch.bmm(grad_output_reshaped, input_batch_unfolded.transpose(1, 2))\n",
    "print(grad_w_mul.shape)\n",
    "\n",
    "# Sum over the batch dimension to aggregate the gradients from each example\n",
    "grad_w = grad_w_mul.sum(dim=0)\n",
    "\n",
    "# Swap axes since the result of bmm doesn't match the (c_out, c_in, k_h, k_w) layout yet\n",
    "grad_w = grad_w.view(c_out, c_in, k_h, k_w)\n",
    "print('Kernel gradient', grad_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q.5. Gradient of the loss w.r.t. input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel reshaped: torch.Size([27, 2])\n",
      "kernel expanded: torch.Size([100, 27, 2])\n",
      "Gradient w.r.t. U shape : torch.Size([100, 27, 25])\n",
      "Gradient w.r.t. input shape torch.Size([100, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Reshape and transpose the kernel\n",
    "kernel_reshaped = kernel.view(c_out, -1).t()\n",
    "print(f'kernel reshaped: {kernel_reshaped.shape}')\n",
    "\n",
    "# Expand kernel to match the batch size\n",
    "kernel_expanded = kernel_reshaped.unsqueeze(0).expand(n, -1, -1)\n",
    "print(f'kernel expanded: {kernel_expanded.shape}')\n",
    "\n",
    "# Perform the batch matrix multiplication -> (n, c_in * k_h * k_w, h * w)\n",
    "grad_U = torch.bmm(kernel_expanded, grad_output_reshaped)\n",
    "print(f'Gradient w.r.t. U shape : {grad_U.shape}')\n",
    "\n",
    "# Use F.fold to reshape the result back to the input shape\n",
    "input_batch_grad = F.fold(grad_U, output_size=(h, w), kernel_size=(k_h, k_w), padding=1, stride=1)\n",
    "print('Gradient w.r.t. input shape', input_batch_grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6 : Implement as Pytorch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DFunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        \n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        \n",
    "        b, c, h, w = input_batch.size()\n",
    "        out_channels, in_channels, kh, kw = kernel.shape\n",
    "        \n",
    "        # Get all input patches\n",
    "        patches = F.unfold(input_batch, (kh, kw), padding= padding, stride=stride)\n",
    "        \n",
    "        # Flatten patches into row vectors\n",
    "        X = patches.transpose(1, 2).reshape(-1, c * kh * kw)\n",
    "        \n",
    "        # Matrix multiplication with the kernel matrix\n",
    "        Y = X.matmul(kernel.view(kernel.size(0), -1).t())\n",
    "        \n",
    "        # Calculate expected output dimensions\n",
    "        expected_h, expected_w = ((h + 2 * padding - kh) // stride + 1,\n",
    "                                  (w + 2 * padding - kw) // stride + 1)\n",
    "        \n",
    "        # Reshape Y to match the expected output dimensions\n",
    "        output = Y.reshape(b, -1, expected_h, expected_w).permute(0, 1, 2, 3)\n",
    "        \n",
    "        # Assert that the output dimensions match the expected dimensions\n",
    "        assert output.shape[2] == expected_h and output.shape[3] == expected_w, \"Output dimensions do not match expected dimensions\"\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve input and kernel from context saved in the forward pass\n",
    "        input_batch, kernel = ctx.saved_tensors\n",
    "        \n",
    "        # Retrieve stride and padding from context\n",
    "        stride, padding = ctx.stride, ctx.padding\n",
    "        c_out, c_in, kh, kw = kernel.shape\n",
    "        \n",
    "        # (Q4) : Calculate gradient w.r.t. kernel using matrix multiplication\n",
    "        # We start with grad_output, which is the gradient of the loss with respect to the layer's output. \n",
    "        # we need to consider how the kernel interacts with the input. \n",
    "        # Since the forward pass involves multiplying the kernel with the input, \n",
    "        # the backward pass involves multiplying the gradient of the output with the input.\n",
    "        # gradients with respect to the kernel for each of these patches are summed up. \n",
    "        # This summation is equivalent to the batch matrix multiplication between the grad_output and the input_batch\n",
    "        \n",
    "        # unfold input\n",
    "        input_batch_unfolded = F.unfold(input_batch, (kh, kw), padding= padding, stride=stride)\n",
    "\n",
    "        # Reshape grad_output to match Y'\n",
    "        # grad_output to shape ready for bmm: (n, c_out, h*w)\n",
    "        grad_output_reshaped = grad_output.view(n, c_out, -1)\n",
    "        \n",
    "        # Perform the batch matrix multiplication\n",
    "        # grad_w_mul shape: (n, c_out, c_in * k_h * k_w)\n",
    "        kernel_grad_mul = torch.bmm(grad_output_reshaped, input_batch_unfolded.transpose(1, 2))\n",
    "\n",
    "        # Sum over the batch dimension to aggregate the gradients from each example\n",
    "        # grad_w shape: (c_out, c_in * k_h * k_w)\n",
    "        kernel_grad_sum = kernel_grad_mul.sum(dim=0)\n",
    "\n",
    "        # Reshape the summed gradient to the shape of the kernel weights\n",
    "        # grad_w shape: (c_out, c_in, k_h, k_w)\n",
    "        kernel_grad = kernel_grad_sum.view(c_out, c_in, kh, kw)\n",
    "        \n",
    "        \n",
    "        # (Q5): Compute gradient w.r.t. input (grad_input)\n",
    "        # We start with grad_output, which is the gradient of the loss with respect to the output \n",
    "        # Need to perform a 'reverse' convolution operation. \n",
    "        # This is achieved using a transposed convolution. \n",
    "        # In transposed convolution, we slide the kernel over the grad_output, \n",
    "        # much like in the forward pass, but in a way that reconstructs the gradient with respect to the input\n",
    "        # For this, we need to 'unfold' the grad_output\n",
    "        # Since the kernel overlaps with multiple regions of the input during the forward pass, \n",
    "        # during the backward pass, the gradients from these overlapping regions are summed up in the input gradient.  \n",
    "        # Then we use the 'fold' operation to map it back to input space\n",
    "        \n",
    "        # Reshape and transpose the kernel\n",
    "        kernel_reshaped = kernel.view(c_out, -1).t()\n",
    "\n",
    "        # Expand kernel to match the batch size\n",
    "        kernel_expanded = kernel_reshaped.unsqueeze(0).expand(n, -1, -1)\n",
    "\n",
    "        # Perform the batch matrix multiplication -> (n, c_in * k_h * k_w, h * w)\n",
    "        grad_U = torch.bmm(kernel_expanded, grad_output_reshaped)\n",
    "\n",
    "        # Use F.fold to reshape the result back to the input shape\n",
    "        input_grad = F.fold(grad_U, output_size=(h, w), kernel_size=(k_h, k_w), padding=1, stride=1)\n",
    "        \n",
    "        return input_grad, kernel_grad, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to the input batch:\n",
      "tensor([[[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]]])\n",
      "Gradient with respect to the kernel:\n",
      "tensor([[[[   5.5613,   31.8693,   34.2729],\n",
      "          [  -3.0881,    3.9458,    4.9485],\n",
      "          [ -12.6810,   -7.0787,   -6.7496]],\n",
      "\n",
      "         [[ -60.9975,  -45.6084,  -57.6345],\n",
      "          [ -68.8964,  -51.1240,  -50.4738],\n",
      "          [ -56.3578,  -59.0504,  -60.1422]],\n",
      "\n",
      "         [[ -50.1311,  -47.3236,  -27.0540],\n",
      "          [ -98.0290,  -95.6402,  -53.6228],\n",
      "          [-116.9863, -113.6081,  -60.5449]]],\n",
      "\n",
      "\n",
      "        [[[   5.5613,   31.8693,   34.2729],\n",
      "          [  -3.0881,    3.9458,    4.9485],\n",
      "          [ -12.6810,   -7.0787,   -6.7496]],\n",
      "\n",
      "         [[ -60.9975,  -45.6085,  -57.6345],\n",
      "          [ -68.8964,  -51.1240,  -50.4738],\n",
      "          [ -56.3578,  -59.0504,  -60.1422]],\n",
      "\n",
      "         [[ -50.1311,  -47.3236,  -27.0540],\n",
      "          [ -98.0291,  -95.6402,  -53.6228],\n",
      "          [-116.9863, -113.6081,  -60.5449]]]])\n"
     ]
    }
   ],
   "source": [
    "# Define the input batch and the kernel with compatible dimensions\n",
    "# Define the input batch and kernel with dimensions\n",
    "in_channels, out_channels, kernel_size = 3, 2, (3, 3)\n",
    "input_batch = torch.randn(100, in_channels, 5, 5, requires_grad=True)\n",
    "kernel = torch.randn(out_channels, in_channels, *kernel_size, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "output = Conv2DFunction.apply(input_batch, kernel)\n",
    "#print(output.shape)\n",
    "\n",
    "# Simulate a gradient output by creating a tensor of ones with the same shape as the output\n",
    "grad_output = torch.ones_like(output)\n",
    "\n",
    "# Get the gradients of the input and kernel by calling backward on the output\n",
    "output.backward(grad_output)\n",
    "\n",
    "input_grad = input_batch.grad\n",
    "kernel_grad = kernel.grad\n",
    "# Print out the gradients\n",
    "print('Gradient with respect to the input batch:')\n",
    "print(input_batch.grad)\n",
    "print('Gradient with respect to the kernel:')\n",
    "print(kernel.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd input gradient:\n",
      "tensor([[[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.8670, -0.9676, -0.9676, -0.9676, -3.5216],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-2.9142, -1.8772, -1.8772, -1.8772, -2.7513],\n",
      "          [-5.6381, -4.3437, -4.3437, -4.3437, -1.7587]],\n",
      "\n",
      "         [[ 0.8276,  3.3160,  3.3160,  3.3160,  2.9666],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 1.6120,  1.8535,  1.8535,  1.8535,  0.1105],\n",
      "          [ 2.8575,  1.5768,  1.5768,  1.5768, -1.0493]],\n",
      "\n",
      "         [[-1.8039, -0.4498, -0.4498, -0.4498,  0.2775],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-2.4076, -0.7794, -0.7794, -0.7794,  1.3656],\n",
      "          [-0.8992, -0.1203, -0.1203, -0.1203,  2.1274]]]])\n",
      "Autograd kernel gradient:\n",
      "tensor([[[[   5.5613,   31.8692,   34.2729],\n",
      "          [  -3.0881,    3.9457,    4.9485],\n",
      "          [ -12.6810,   -7.0787,   -6.7496]],\n",
      "\n",
      "         [[ -60.9975,  -45.6085,  -57.6345],\n",
      "          [ -68.8964,  -51.1240,  -50.4738],\n",
      "          [ -56.3578,  -59.0504,  -60.1422]],\n",
      "\n",
      "         [[ -50.1311,  -47.3236,  -27.0540],\n",
      "          [ -98.0290,  -95.6402,  -53.6228],\n",
      "          [-116.9862, -113.6081,  -60.5449]]],\n",
      "\n",
      "\n",
      "        [[[   5.5613,   31.8692,   34.2729],\n",
      "          [  -3.0881,    3.9457,    4.9485],\n",
      "          [ -12.6810,   -7.0787,   -6.7496]],\n",
      "\n",
      "         [[ -60.9975,  -45.6085,  -57.6345],\n",
      "          [ -68.8964,  -51.1240,  -50.4738],\n",
      "          [ -56.3578,  -59.0504,  -60.1422]],\n",
      "\n",
      "         [[ -50.1311,  -47.3236,  -27.0540],\n",
      "          [ -98.0290,  -95.6402,  -53.6228],\n",
      "          [-116.9862, -113.6081,  -60.5449]]]])\n",
      "Gradients computed correctly!\n"
     ]
    }
   ],
   "source": [
    "# Now compare with PyTorch's autograd\n",
    "# Reset gradients\n",
    "input_batch.grad.zero_()\n",
    "kernel.grad.zero_()\n",
    "\n",
    "# Using PyTorch's Conv2d for comparison\n",
    "conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv_layer.weight.copy_(kernel)\n",
    "    conv_layer.bias.zero_()  # Assuming zero bias for comparison\n",
    "\n",
    "output_pytorch = conv_layer(input_batch)\n",
    "output_pytorch.backward(grad_output)\n",
    "\n",
    "# Print gradients from PyTorch's autograd\n",
    "print('Autograd input gradient:')\n",
    "print(input_batch.grad)\n",
    "print('Autograd kernel gradient:')\n",
    "print(conv_layer.weight.grad)\n",
    "\n",
    "# Check if the gradients match to a reasonable degree of precision\n",
    "assert torch.allclose(input_batch.grad, input_grad, atol=1e-5), \"Input gradients do not match\"\n",
    "assert torch.allclose(kernel.grad, kernel_grad, atol=1e-5), \"Kernel gradients do not match\"\n",
    "\n",
    "print(\"Gradients computed correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison with autograd built-in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to the input batch:\n",
      "torch.Size([16, 3, 32, 32])\n",
      "Gradient with respect to the kernel:\n",
      "torch.Size([8, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2DFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward\n",
    "    passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input\n",
    "        and return a Tensor containing the output. ctx is a context\n",
    "        object that can be used to stash information for backward\n",
    "        computation. You can cache arbitrary objects for use in the\n",
    "        backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        # store objects and parameters for the backward\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        \n",
    "        # Perform the convolution operation\n",
    "        output_batch = F.conv2d(input_batch, kernel, stride=stride, padding=padding)\n",
    "        \n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the\n",
    "        gradient of the loss with respect to the output, and we need\n",
    "        to compute the gradient of the loss with respect to the\n",
    "        input.\n",
    "        \"\"\"\n",
    "        # retrieve stored objects\n",
    "        input_batch, kernel = ctx.saved_tensors\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "        \n",
    "        # Compute gradients with respect to the input and kernel\n",
    "        input_batch_grad = F.grad.conv2d_input(input_batch.shape, kernel, grad_output, stride=stride, padding=padding)\n",
    "        kernel_grad = F.grad.conv2d_weight(input_batch, kernel.shape, grad_output, stride=stride, padding=padding)\n",
    "        \n",
    "        # Return the gradients, with None for the stride and padding\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "\n",
    "# Define the input batch and kernel with dimensions\n",
    "in_channels, out_channels, kernel_size = 3, 8, (3, 3)\n",
    "input_batch = torch.randn(16, in_channels, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(out_channels, in_channels, *kernel_size, requires_grad=True)\n",
    "\n",
    "# Apply the custom convolution function\n",
    "output_batch = Conv2DFunc.apply(input_batch, kernel)\n",
    "\n",
    "# Pretend we have some gradient coming back during backpropagation\n",
    "grad_output = torch.randn_like(output_batch)\n",
    "\n",
    "# Get the gradients of the input and kernel by calling backward on the output\n",
    "output_batch.backward(grad_output)\n",
    "\n",
    "# Print out the gradients\n",
    "print('Gradient with respect to the input batch:')\n",
    "print(input_batch.grad.shape)\n",
    "print('Gradient with respect to the kernel:')\n",
    "print(kernel.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End part 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
