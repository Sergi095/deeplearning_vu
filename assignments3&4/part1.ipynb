{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Implementing a convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a non-vectorized implementation. The convolution works as follows: given an input tensor x with dimensions\n",
    "(batch_size, input_channels, input_width, input_height), an amount of padding, a number of output channels, kernel size, and a stride, we produce an output tensor with dimensions (batch_size, output_channels, output_width, output_height)\n",
    "\n",
    "Question 1: Write a pseudo-code for how you would implement this with a set of nested\n",
    "for loops. The convolution is defined by a set of weights/parameters which we will learn.\n",
    "How do you represent these weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Pseudocode for convolution (non-vectorized)\n",
    "- Weights are represented as a 4D tensor with dimensions (output_channels, input_channels, kernel_width, kernel_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'weights' is a 4D tensor with dimensions :\n",
    "#(output_channels, input_channels, kernel_height,kernel_width)\n",
    "\n",
    "Function convolution(input_tensor, weights, bias, padding, stride):\n",
    "    '''\n",
    "    Convolve the input tensor with the given filter weights \n",
    "    input_tensor: 4D tensor with dimensions \n",
    "    (batch_size, input_channels,\n",
    "    input_height, input_width)\n",
    "    weights: 4D tensor (output_channels, \n",
    "    input_channels, \n",
    "    kernel_height, kernel_width)\n",
    "    bias: 1D tensor (output_channels)\n",
    "    padding: int\n",
    "    stride: int\n",
    "    '''\n",
    "    \n",
    "    # Extract input dimensions\n",
    "    batch_size, \\\n",
    "    input_channels, \\\n",
    "    input_height, input_width = dimensions(input_tensor)\n",
    "\n",
    "    # Calculate dimensions of the output tensor\n",
    "    output_height, output_width = compute_output_size((input_height, input_width), \\\n",
    "    (kernel_height, kernel_width), stride, padding)\n",
    "    output_channels = dimensions(weights)[0]\n",
    "\n",
    "    # Initialize the output tensor with zeros\n",
    "    output_tensor = zeros(batch_size, output_channels, output_height, output_width)\n",
    "\n",
    "    # Apply padding to the input tensor\n",
    "    padded_input = apply_padding(input_tensor, padding)\n",
    "\n",
    "    # Loop over every example in the batch\n",
    "    For b in range(batch_size):\n",
    "        # Loop over every output channel\n",
    "        For oc in range(output_channels):\n",
    "            # Loop over the output spatial dimensions\n",
    "            For oh in range(output_height):\n",
    "                For ow in range(output_width):\n",
    "                    # Initialize a variable to store the convolved value\n",
    "                    convolved_value = 0\n",
    "                    \n",
    "                    # Iterate over each input channel\n",
    "                    For ic in range(input_channels):\n",
    "                        # Iterate over the kernel's spatial dimensions\n",
    "                        For kh in range(kernel_height):\n",
    "                            For kw in range(kernel_width):\n",
    "                                # Calculate the indices on the padded input\n",
    "                                i = oh * stride + kh\n",
    "                                j = ow * stride + kw\n",
    "                                \n",
    "                                # Accumulate the weighted sum for the convolution\n",
    "                                convolved_value += padded_input[b, ic, i, j] *\n",
    "                                                   weights[oc, ic, kh, kw]\n",
    "\n",
    "                    # Store the output \n",
    "                    output_tensor[b, oc, oh, ow] = convolved_value \n",
    "\n",
    "    Return output_tensor\n",
    "\n",
    "Function apply_padding(input_tensor, padding):\n",
    "    # Apply zero-padding to the input tensor\n",
    "    # ...\n",
    "\n",
    "Function compute_output_size(input_size, kernel_size, stride, padding):\n",
    "    # Calculate the height and width of the output tensor\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1.Function to compute the output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8, 32, 32)\n",
      "torch.Size([100, 8, 32, 32])\n",
      "The output size is correct.\n"
     ]
    }
   ],
   "source": [
    "def compute_output_size(input_tensor, filters, stride, padding):\n",
    "    \"\"\"\n",
    "    Computes the output size of a convolutional operation for a batch of images.\n",
    "\n",
    "    :param input_tensor: 4D tensor with shape (batch_size, input_channels, input_height, input_width)\n",
    "    :param filters: 4D tensor with shape (output_channels, input_channels, filter_height, filter_width)\n",
    "    :param stride: Integer representing the stride of the convolution.\n",
    "    :param padding: Integer representing the amount of padding added to the input tensor.\n",
    "\n",
    "    :return: Tuple (batch_size, output_channels, output_width, output_height) representing the dimensions of the output tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract batch size and input channels from input tensor, and output channels from filters\n",
    "    batch_size, input_channels = input_tensor.shape[0], input_tensor.shape[1]\n",
    "    output_channels = filters.shape[0]\n",
    "\n",
    "    # Extract the spatial dimensions of the input tensor\n",
    "    input_height, input_width = input_tensor.shape[2], input_tensor.shape[3]\n",
    "\n",
    "    # Extract the spatial dimensions of the filters\n",
    "    filter_height, filter_width = filters.shape[2], filters.shape[3]\n",
    "\n",
    "    # Compute the output spatial dimensions\n",
    "    output_height = ((input_height - filter_height + 2 * padding) // stride) + 1\n",
    "    output_width = ((input_width - filter_width + 2 * padding) // stride) + 1\n",
    "    \n",
    "    # Return the dimensions of the output tensor\n",
    "    return (batch_size, output_channels, output_width, output_height)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming input_tensor and filters are numpy arrays with the correct 4D shapes\n",
    "# and stride and padding are integers.\n",
    "\n",
    "filters = np.random.rand(8, 3, 3, 3) # 8 filters, 3 channels, 3x3 kernel\n",
    "input_tensor = np.random.rand(100, 4, 5, 5) # 100 images, 3 channels, 32x32 input\n",
    "stride = 1\n",
    "padding = 1\n",
    "output_size = compute_output_size(input_tensor, filters, stride, padding)\n",
    "\n",
    "# Actual output size from PyTorch's convolution operation\n",
    "inputs = torch.randn(100, 3, 32, 32)\n",
    "filters = torch.randn(8, 3, 3, 3) \n",
    "actual_output_size = F.conv2d(inputs, filters, padding=padding, stride=stride).shape\n",
    "\n",
    "# Expected output size from our custom compute_output_size function\n",
    "expected_output_size = compute_output_size(inputs, filters, stride, padding)\n",
    "print(expected_output_size)\n",
    "print(actual_output_size)\n",
    "# Convert PyTorch shape to a tuple for comparison\n",
    "actual_output_size = tuple(actual_output_size)\n",
    "\n",
    "# Assert that the sizes match\n",
    "assert actual_output_size == expected_output_size, f\"Expected {expected_output_size}, got {actual_output_size}\"\n",
    "print(\"The output size is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2.vectorized implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract all patches from the input\n",
    "2. Flatten these patches (with all channels) into vectors, arranged as the rows of a\n",
    "matrix X.\n",
    "3. Multiply this matrix by a weight matrix Y = XW.\n",
    "4. Reshape the matrix Y, so that its rows become the pixels of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches (X): torch.Size([100, 27, 1024])\n",
      "Flattened patches (X): torch.Size([102400, 27])\n",
      "Reshaped Kernel (Y): torch.Size([27, 8])\n",
      "Output from Matrix multiplication (Y): torch.Size([102400, 8])\n",
      "Recovered output shape (Y): torch.Size([100, 8, 32, 32])\n",
      "The output size matches the expected size.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def vectorized_convolution(input_tensor, weight, stride=1, padding=1):\n",
    "    # Step 1: Extract all patches from the input tensor\n",
    "    # The size of the weight determines the kernel_size\n",
    "    kernel_size = (weight.shape[2], weight.shape[3])\n",
    "    patches = F.unfold(\n",
    "        input_tensor, \n",
    "        kernel_size=kernel_size, \n",
    "        stride=stride, \n",
    "        padding=padding\n",
    "    )\n",
    "    print(f'Patches (X): {patches.shape}')\n",
    "    \n",
    "    # Step 2: Flatten these patches\n",
    "    # 'unfold' output has shape (batch_size, C * kernel_height * kernel_width, L)\n",
    "    X = patches.transpose(1, 2).reshape(\n",
    "        -1, \n",
    "        weight.shape[1] * kernel_size[0] * kernel_size[1]\n",
    "    )\n",
    "    print(f'Flattened patches (X): {X.shape}')\n",
    "    \n",
    "    # Reshape weights to match the flattened patches\n",
    "    W = weight.reshape(weight.shape[0], -1).t()\n",
    "    print(f'Reshaped Kernel (Y): {W.shape}')\n",
    "    \n",
    "    # Step 3: Multiply by the weight matrix\n",
    "    Y = torch.matmul(X, W)\n",
    "    print(f'Output from Matrix multiplication (Y): {Y.shape}')\n",
    "    \n",
    "    # Step 4: Reshape the matrix Y so that its rows become the pixels of the output tensor\n",
    "    # The output shape will have the following dimensions (batch_size, out_channels, out_height, out_width)\n",
    "    output_height = (input_tensor.shape[2] + 2 * padding - kernel_size[0]) // stride + 1\n",
    "    output_width = (input_tensor.shape[3] + 2 * padding - kernel_size[1]) // stride + 1\n",
    "    output = Y.reshape(\n",
    "        input_tensor.shape[0], \n",
    "        output_height, \n",
    "        output_width, \n",
    "        -1\n",
    "    ).permute(0, 3, 1, 2)\n",
    "    print(f'Recovered output shape (Y): {output.shape}')\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "batch_size = 100\n",
    "in_channels = 3\n",
    "height, width = 32, 32\n",
    "out_channels = 8\n",
    "kernel_height, kernel_width = 3, 3\n",
    "\n",
    "input_tensor = torch.randn(batch_size, in_channels, height, width)\n",
    "weight = torch.randn(out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "output_tensor = vectorized_convolution(input_tensor, weight, stride=1, padding=1)\n",
    "\n",
    "# Check that output size is correct\n",
    "assert output_tensor.shape == (batch_size, out_channels, height, width), \"The output size does not match the expected size.\"\n",
    "\n",
    "print(\"The output size matches the expected size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.1. Unfold pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function unfold(input_tensor, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Extracts sliding local blocks from a batched input tensor.\n",
    "\n",
    "    Parameters:\n",
    "    input_tensor: A 4D tensor of shape (batch_size, channels, height, width).\n",
    "    kernel_size: A tuple (kH, kW) representing the height and width of the kernel.\n",
    "    stride: A tuple (sH, sW) representing the vertical and horizontal strides.\n",
    "    padding: A tuple (pH, pW) representing the padding added to the height and width.\n",
    "\n",
    "    Returns:\n",
    "    A 3D tensor where each patch is flattened into a row vector.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, channels, height, width = get_shape(input_tensor)\n",
    "    kH, kW = kernel_size\n",
    "    sH, sW = stride\n",
    "    pH, pW = padding\n",
    "\n",
    "    # Apply padding to the input tensor\n",
    "    padded_tensor = apply_padding(input_tensor, pH, pW)\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = (height + 2 * pH - kH) // sH + 1\n",
    "    output_width = (width + 2 * pW - kW) // sW + 1\n",
    "\n",
    "    # Initialize an empty list to store the patches\n",
    "    patches = []\n",
    "\n",
    "    # Loop over every example in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Loop over the output spatial dimensions\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                # Calculate the starting and ending indices of the patch\n",
    "                start_h = h * sH\n",
    "                start_w = w * sW\n",
    "                end_h = start_h + kH\n",
    "                end_w = start_w + kW\n",
    "\n",
    "                # Extract the patch and flatten it\n",
    "                patch = flattened_patch(padded_tensor[i, :, start_h:end_h, start_w:end_w])\n",
    "                patches.append(patch)\n",
    "\n",
    "    # Reshape the list of patches into a 3D tensor \n",
    "    # batch_size, input_channels * kernel_height * kernel_width, number_of_patches\n",
    "    output_tensor = reshape_into_3D_tensor(patches, batch_size, output_height, output_width)\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.2.Pytorch Module implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 27, 1024])\n",
      "torch.Size([102400, 27])\n",
      "torch.Size([102400, 8])\n",
      "torch.Size([100, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.kernel = torch.randn(out_channels, in_channels * kernel_size[0] * kernel_size[1])\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        b, c, h, w = input_batch.size()\n",
    "\n",
    "        # Get all input patches\n",
    "        patches = F.unfold(input_batch, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        # print(patches.shape)\n",
    "\n",
    "        # Flatten patches into row vectors and treat both b and p as batch dimensions\n",
    "        # Reshape patches to a (b*p, k) tensor for batched matrix multiplication\n",
    "        X = patches.transpose(1, 2).reshape(-1, c * self.kernel_size[0] * self.kernel_size[1])\n",
    "        #print(X.shape)\n",
    "        \n",
    "        # Matrix multiplication with the kernel matrix\n",
    "        Y = X.matmul(self.kernel.t())\n",
    "        #print(Y.shape)\n",
    "\n",
    "        # Calculate expected output dimensions\n",
    "        expected_h, expected_w = ((h + 2 * self.padding - self.kernel_size[0]) // self.stride + 1,\n",
    "                                  (w + 2 * self.padding - self.kernel_size[1]) // self.stride + 1)\n",
    "\n",
    "        # Reshape Y back from (b*p, k) to ouput tensor dimensions\n",
    "        output = Y.reshape(b, expected_h, expected_w, self.out_channels).permute(0, 3, 1, 2)\n",
    "        #print(output.shape)\n",
    "\n",
    "        # Assert the output dimensions\n",
    "        assert output.shape[1:] == (self.out_channels, expected_h, expected_w), \"Output dimensions do not match expected dimensions\"\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "in_channels = 3\n",
    "out_channels = 8\n",
    "conv = Conv2D(in_channels, out_channels)\n",
    "input_batch = torch.randn(100, in_channels, 32, 32)\n",
    "output_batch = conv(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.3. Pytorch Function implementation (see Q4 for backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102400, 8])\n",
      "torch.Size([100, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class Conv2DFunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        \n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        \n",
    "        b, c, h, w = input_batch.size()\n",
    "        out_channels, in_channels, kh, kw = kernel.shape\n",
    "        \n",
    "        # Get all input patches\n",
    "        patches = F.unfold(input_batch, (kh, kw), padding= padding, stride=stride)\n",
    "        \n",
    "        # Flatten patches into row vectors\n",
    "        X = patches.transpose(1, 2).reshape(-1, c * kh * kw)\n",
    "        \n",
    "        # Matrix multiplication with the kernel matrix\n",
    "        Y = X.matmul(kernel.view(kernel.size(0), -1).t())\n",
    "        print(Y.shape)\n",
    "        \n",
    "        # Calculate expected output dimensions\n",
    "        expected_h, expected_w = ((h + 2 * padding - kh) // stride + 1,\n",
    "                                  (w + 2 * padding - kw) // stride + 1)\n",
    "        \n",
    "        # Reshape Y to match the expected output dimensions\n",
    "        output = Y.reshape(b, -1, expected_h, expected_w).permute(0, 1, 2, 3)\n",
    "        \n",
    "        # Assert that the output dimensions match the expected dimensions\n",
    "        assert output.shape[2] == expected_h and output.shape[3] == expected_w, \"Output dimensions do not match expected dimensions\"\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# Define the number of input and output channels\n",
    "input_batch = torch.randn(100, 3, 32, 32)\n",
    "kernel = torch.randn(out_channels, in_channels, 3, 3)\n",
    "output = Conv2DFunction.apply(input_batch, kernel)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q.4 Gradient of the loss w.r.t. weights \n",
    "- Q.5. Gradient of the loss w.r.t. input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 25])\n",
      "torch.Size([100, 27, 25])\n",
      "torch.Size([100, 2, 27])\n",
      "torch.Size([2, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define dimensions and create random tensors for input_batch and grad_output\n",
    "n, c_in, h, w = 100, 3, 5, 5  # Example dimensions\n",
    "c_out, k_h, k_w = 2, 3, 3  # Kernel dimensions\n",
    "\n",
    "# Create random Tensors for input_batch, grad_output, and kernel\n",
    "input_batch = torch.randn(n, c_in, h, w, requires_grad=True)\n",
    "grad_output = torch.randn(n, c_out, h, w)  # Assuming h_out and w_out are same as h and w for simplicity\n",
    "kernel = torch.randn(c_out, c_in, k_h, k_w, requires_grad=True)\n",
    "\n",
    "# Perform batch matrix multiplication to get the gradient with respect to the kernel weights.\n",
    "# Reshape grad_output as if it's the result of an im2col operation (n, c_out, h_out*w_out)\n",
    "grad_output_reshaped = grad_output.view(n, c_out, -1)\n",
    "print(grad_output_reshaped.shape)\n",
    "\n",
    "# Reshape input_batch to (n, h*w, c_in) to perform batch matrix multiplication with grad_output_reshaped\n",
    "input_batch_unfolded = F.unfold(input_batch, (k_h, k_w), stride=1, padding=1)\n",
    "print(input_batch_unfolded.shape)\n",
    "\n",
    "# Perform the batch matrix multiplication\n",
    "grad_w_mul = torch.bmm(grad_output_reshaped, input_batch_unfolded.transpose(1, 2))\n",
    "print(grad_w_mul.shape)\n",
    "\n",
    "# Sum over the batch dimension to aggregate the gradients from each example\n",
    "grad_w = grad_w_mul.sum(dim=0)\n",
    "\n",
    "# We need to swap axes since the result of bmm doesn't match the (c_out, c_in, k_h, k_w) layout yet\n",
    "# grad_w = grad_w_summed.transpose(0, 1).contiguous()\n",
    "# print(grad_w.shape)\n",
    "grad_w = grad_w.view(c_out, c_in, k_h, k_w)\n",
    "print(grad_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape grad_output to match Y'\n",
    "grad_output_reshaped = grad_output.reshape(grad_output.size(0), grad_output.size(1), -1)\n",
    "print(f'Reshaped output : {grad_output_reshaped.shape}')\n",
    "        \n",
    "# Prepare input_batch for matrix multiplication with grad_output\n",
    "# Reshape input_batch to match the dimensions for matrix multiplication\n",
    "input_reshaped = input_batch.reshape(input_batch.size(0),-1,input_batch.size(2) * input_batch.size(3))   \n",
    "print(f'Reshaped input: {input_reshaped.shape}')\n",
    "print((input_reshaped.transpose(1, 2)).sum(0).shape)\n",
    "        \n",
    "        #print(f'Before bmm {input_reshaped.transpose(1, 2).sum(0).reshape_as(kernel)}:')\n",
    "        # Matrix multiplication to compute the gradient w.r.t. kernel weights\n",
    "        # Transposing input_reshaped for proper matrix multiplication orientation\n",
    "        #kernel_grad = grad_output_reshaped.bmm(input_reshaped.transpose(1, 2)).sum(0).reshape(kernel)\n",
    "        #print(f'Shape kernel : {kernel_grad.shape}')\n",
    "\n",
    "\n",
    "        # Calculate the necessary shape for input_batch reshaping\n",
    "        # The shape of input_reshaped should facilitate the matrix multiplication with grad_output_reshaped\n",
    "        #input_reshaped = input_batch.reshape(input_batch.size(0), input_batch.size(1), -1)\n",
    "\n",
    "        # Perform batch matrix multiplication\n",
    "        # Note: Adjust the transpose dimensions if necessary based on the actual multiplication requirements\n",
    "        #kernel_grad = torch.bmm(grad_output_reshaped, input_reshaped.transpose(1, 2))\n",
    "        \n",
    "        #print(kernel_grad.shape)\n",
    "\n",
    "        # Sum over the batch and reshape to match the kernel shape\n",
    "        #kernel_grad = kernel_grad.sum(0).reshape_as(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DFunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        \n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        \n",
    "        b, c, h, w = input_batch.size()\n",
    "        out_channels, in_channels, kh, kw = kernel.shape\n",
    "        \n",
    "        # Get all input patches\n",
    "        patches = F.unfold(input_batch, (kh, kw), padding= padding, stride=stride)\n",
    "        \n",
    "        # Flatten patches into row vectors\n",
    "        X = patches.transpose(1, 2).reshape(-1, c * kh * kw)\n",
    "        \n",
    "        # Matrix multiplication with the kernel matrix\n",
    "        Y = X.matmul(kernel.view(kernel.size(0), -1).t())\n",
    "        \n",
    "        # Calculate expected output dimensions\n",
    "        expected_h, expected_w = ((h + 2 * padding - kh) // stride + 1,\n",
    "                                  (w + 2 * padding - kw) // stride + 1)\n",
    "        \n",
    "        # Reshape Y to match the expected output dimensions\n",
    "        output = Y.reshape(b, -1, expected_h, expected_w).permute(0, 1, 2, 3)\n",
    "        \n",
    "        # Assert that the output dimensions match the expected dimensions\n",
    "        assert output.shape[2] == expected_h and output.shape[3] == expected_w, \"Output dimensions do not match expected dimensions\"\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve input and kernel from context saved in the forward pass\n",
    "        input_batch, kernel = ctx.saved_tensors\n",
    "        \n",
    "        # Retrieve stride and padding from context\n",
    "        stride, padding = ctx.stride, ctx.padding\n",
    "        out_channels, in_channels, kh, kw = kernel.shape\n",
    "        \n",
    "        # (Q4) : Calculate gradient w.r.t. kernel using matrix multiplication\n",
    "        # We start with grad_output, which is the gradient of the loss with respect to the layer's output. \n",
    "        # we need to consider how the kernel interacts with the input. \n",
    "        # Since the forward pass involves multiplying the kernel with the input, \n",
    "        # the backward pass involves multiplying the gradient of the output with the input.\n",
    "        # gradients with respect to the kernel for each of these patches are summed up. \n",
    "        # This summation is equivalent to the batch matrix multiplication between the grad_output and the input_batch\n",
    "        \n",
    "        # unfold input\n",
    "        input_batch_unfolded = F.unfold(input_batch, (k_h, k_w), stride=1)\n",
    "\n",
    "        # Reshape grad_output to match Y'\n",
    "        # grad_output to shape ready for bmm: (n, c_out, h*w)\n",
    "        grad_output_reshaped = grad_output.view(n, c_out, -1)\n",
    "\n",
    "        # Perform the batch matrix multiplication\n",
    "        # grad_w_mul shape: (n, c_out, c_in * k_h * k_w)\n",
    "        grad_w_mul = torch.bmm(grad_output_reshaped, input_batch_unfolded.transpose(1, 2))\n",
    "\n",
    "        # Sum over the batch dimension to aggregate the gradients from each example\n",
    "        # grad_w shape: (c_out, c_in * k_h * k_w)\n",
    "        grad_w = grad_w_mul.sum(dim=0)\n",
    "\n",
    "        # Reshape the summed gradient to the shape of the kernel weights\n",
    "        # grad_w shape: (c_out, c_in, k_h, k_w)\n",
    "        grad_w = grad_w.view(c_out, c_in, k_h, k_w)\n",
    "        \n",
    "        \n",
    "        # (Q5): Compute gradient w.r.t. input (grad_input)\n",
    "        # We start with grad_output, which is the gradient of the loss with respect to the output \n",
    "        # Need to perform a 'reverse' convolution operation. \n",
    "        # This is achieved using a transposed convolution. \n",
    "        # In transposed convolution, we slide the kernel over the grad_output, \n",
    "        # much like in the forward pass, but in a way that reconstructs the gradient with respect to the input\n",
    "        # For this, we need to 'unfold' the grad_output\n",
    "        # Since the kernel overlaps with multiple regions of the input during the forward pass, \n",
    "        # during the backward pass, the gradients from these overlapping regions are summed up in the input gradient.  \n",
    "        # Then we use the 'fold' operation to map it back to input space\n",
    "        \n",
    "             \n",
    "        # Reshape and transpose the kernel\n",
    "        #kernel_transposed = kernel.reshape(kernel.size(0), -1).transpose(0, 1)\n",
    "        #print(f'kernel transposed : {kernel_transposed.shape}')\n",
    "        \n",
    "        # Reshape grad_output as grad_output_reshaped to match Y'\n",
    "        #grad_output_unfolded = F.unfold(grad_output, (kh, kw), padding=padding, stride=stride)\n",
    "        \n",
    "        #grad_output_reshaped = grad_output.reshape(grad_output.size(0),\n",
    "        #                                           grad_output.size(1), \n",
    "        #                                           -1)\n",
    "        #print(f'grad_output  unfolded : {grad_output_unfolded.shape}')\n",
    "        \n",
    "        #grad_output_reshaped = grad_output_unfolded.transpose(1, 2).reshape(-1, in_channels * kh * kw)\n",
    "        #print(f'grad_output reshaped : {grad_output_unfolded.shape}')\n",
    "        \n",
    "        # Matrix multiplication with the transposed kernel to get the gradient w.r.t. U\n",
    "        #grad_U = grad_output_reshaped.matmul(kernel_transposed)\n",
    "        #print(f'Gradient w.r.t. U shape : {grad_U.shape}')\n",
    "\n",
    "        # Fold the gradient w.r.t. U back to the shape of the input_batch to get grad_input\n",
    "        #input_batch_grad = F.fold(grad_U, \n",
    "        #                        (h, w), \n",
    "        #                        kernel_size=(kh, kw), \n",
    "        #                        padding=padding, \n",
    "        #                        stride=stride)\n",
    "        #print(f'Input batch gradient shape : {input_batch_grad.shape}')\n",
    "        \n",
    "        # Compute gradient w.r.t. input (grad_input)\n",
    "        # Reshape grad_output to the shape of 'Y' in the forward pass\n",
    "        grad_Y = grad_output.permute(0, 2, 3, 1).reshape(-1, kernel.size[0])\n",
    "        print(grad_Y.shape)\n",
    "        # Perform the transpose convolution operation using the original kernel, which has been flipped\n",
    "        grad_input_padded = grad_Y.matmul(kernel.reshape(kernel.size[0], -1))\n",
    "        print(grad_input_padded.shape)\n",
    "        # Fold back to the original input dimensions\n",
    "        grad_input_padded = grad_input_padded.view(input_batch.size(0), input_batch.size(2) + 2 * padding - 2, input_batch.size(3) + 2 * padding - 2, -1)\n",
    "        grad_input = F.fold(grad_input_padded, (input_batch.size(2), input_batch.size(3)), (1, 1))\n",
    "        print(grad_input.shape)\n",
    "\n",
    "        \n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped output : torch.Size([100, 8, 1024])\n",
      "Reshaped input: torch.Size([100, 3, 1024])\n",
      "torch.Size([1024, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-4ce5ba80b874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Get the gradients of the input and kernel by calling backward on the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Print out the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                \"of them.\")\n\u001b[1;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-292-42ac4d7ce5e1>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Compute gradient w.r.t. input (grad_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Reshape grad_output to the shape of 'Y' in the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mgrad_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Perform the transpose convolution operation using the original kernel, which has been flipped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Define the input batch and the kernel with compatible dimensions\n",
    "# Define the input batch and kernel with dimensions\n",
    "in_channels, out_channels, kernel_size = 3, 8, (3, 3)\n",
    "input_batch = torch.randn(100, in_channels, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(out_channels, in_channels, *kernel_size, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "output = Conv2DFunction.apply(input_batch, kernel)\n",
    "#print(output.shape)\n",
    "\n",
    "# Fake a gradient output by creating a tensor of ones with the same shape as the output\n",
    "grad_output = torch.ones_like(output)\n",
    "#print(grad_output.shape)\n",
    "\n",
    "# Get the gradients of the input and kernel by calling backward on the output\n",
    "output.backward(grad_output)\n",
    "\n",
    "# Print out the gradients\n",
    "print('Gradient with respect to the input batch:')\n",
    "print(input_batch.grad)\n",
    "print('Gradient with respect to the kernel:')\n",
    "print(kernel.grad)\n",
    "\n",
    "\n",
    "\n",
    "# Backward pass\n",
    "#output.backward(output, grad_output)\n",
    "\n",
    "#print('Input gradient shape:', input_grad.shape)\n",
    "#print('Kernel gradient shape:', kernel_grad.shape)\n",
    "\n",
    "# Verify the gradients by comparing with the PyTorch's autograd\n",
    "# output.backward(grad_output)\n",
    "\n",
    "#print('Autograd input gradient shape:', input_batch.grad.shape)\n",
    "#print('Autograd kernel gradient shape:', kernel.grad.shape)\n",
    "\n",
    "# Check if the gradients match to a reasonable degree of precision\n",
    "#assert torch.allclose(input_batch.grad, input_grad, atol=1e-5), \"Input gradients do not match\"\n",
    "#assert torch.allclose(kernel.grad, kernel_grad, atol=1e-5), \"Kernel gradients do not match\"\n",
    "\n",
    "#print(\"Gradients computed correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison with autograd already defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to the input batch:\n",
      "torch.Size([16, 3, 32, 32])\n",
      "Gradient with respect to the kernel:\n",
      "torch.Size([8, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2DFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward\n",
    "    passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input\n",
    "        and return a Tensor containing the output. ctx is a context\n",
    "        object that can be used to stash information for backward\n",
    "        computation. You can cache arbitrary objects for use in the\n",
    "        backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        # store objects and parameters for the backward\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        \n",
    "        # Perform the convolution operation\n",
    "        output_batch = F.conv2d(input_batch, kernel, stride=stride, padding=padding)\n",
    "        \n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the\n",
    "        gradient of the loss with respect to the output, and we need\n",
    "        to compute the gradient of the loss with respect to the\n",
    "        input.\n",
    "        \"\"\"\n",
    "        # retrieve stored objects\n",
    "        input_batch, kernel = ctx.saved_tensors\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "        \n",
    "        # Compute gradients with respect to the input and kernel\n",
    "        input_batch_grad = F.grad.conv2d_input(input_batch.shape, kernel, grad_output, stride=stride, padding=padding)\n",
    "        kernel_grad = F.grad.conv2d_weight(input_batch, kernel.shape, grad_output, stride=stride, padding=padding)\n",
    "        \n",
    "        # Return the gradients, with None for the stride and padding\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "\n",
    "# Define the input batch and kernel with dimensions\n",
    "in_channels, out_channels, kernel_size = 3, 8, (3, 3)\n",
    "input_batch = torch.randn(16, in_channels, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(out_channels, in_channels, *kernel_size, requires_grad=True)\n",
    "\n",
    "# Apply the custom convolution function\n",
    "output_batch = Conv2DFunc.apply(input_batch, kernel)\n",
    "\n",
    "# Pretend we have some gradient coming back during backpropagation\n",
    "grad_output = torch.randn_like(output_batch)\n",
    "\n",
    "# Get the gradients of the input and kernel by calling backward on the output\n",
    "output_batch.backward(grad_output)\n",
    "\n",
    "# Print out the gradients\n",
    "print('Gradient with respect to the input batch:')\n",
    "print(input_batch.grad.shape)\n",
    "print('Gradient with respect to the kernel:')\n",
    "print(kernel.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-27771fe6a8cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0minput_batch_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2DFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input gradient shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batch_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class Conv2DFunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        \n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "\n",
    "        output_batch = F.conv2d(input_batch, kernel, stride=stride, padding=padding)\n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_batch, kernel = ctx.saved_tensors\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "\n",
    "        input_batch_grad = kernel_grad = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            input_batch_grad = F.grad.conv2d_input(input_batch.shape, kernel, grad_output, stride, padding)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            kernel_grad = F.grad.conv2d_weight(input_batch, kernel.shape, grad_output, stride, padding)\n",
    "        \n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "\n",
    "# Define the number of input and output channels\n",
    "in_channels = 3\n",
    "out_channels = 8\n",
    "\n",
    "input_batch = torch.randn(100, in_channels, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(out_channels, in_channels, 3, 3, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "output_batch = Conv2DFunction.apply(input_batch, kernel)\n",
    "\n",
    "# Fake a gradient output by creating a tensor of ones with the same shape as the output\n",
    "grad_output = torch.ones_like(output_batch)\n",
    "\n",
    "# Backward pass\n",
    "input_batch_grad, kernel_grad = Conv2DFunction.apply(input_batch, kernel).backward(grad_output)\n",
    "\n",
    "print('Input gradient shape:', input_batch_grad.shape)\n",
    "print('Kernel gradient shape:', kernel_grad.shape)\n",
    "print('Input gradient:', input_batch_grad)\n",
    "print('Kernel gradient:', kernel_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'conv2d_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-faf49b83a51b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0minput_batch_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2DFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input gradient shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batch_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                \"of them.\")\n\u001b[1;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-218-faf49b83a51b>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0minput_batch_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mkernel_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'conv2d_input'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class Conv2DFunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        ctx.save_for_backward(input_batch, kernel)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "\n",
    "        output_batch = F.conv2d(input_batch, kernel, stride=stride, padding=padding)\n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_batch, kernel = ctx.saved_tensors\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "\n",
    "        input_batch_grad = kernel_grad = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            input_batch_grad = F.conv2d_input(input_batch.shape, kernel, grad_output, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            kernel_grad = F.conv2d_weight(input_batch, kernel.shape, grad_output, stride=stride, padding=padding)\n",
    "        \n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "\n",
    "# Define the number of input and output channels\n",
    "in_channels = 3\n",
    "out_channels = 8\n",
    "\n",
    "input_batch = torch.randn(100, in_channels, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(out_channels, in_channels, 3, 3, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "output_batch = Conv2DFunction.apply(input_batch, kernel)\n",
    "\n",
    "# Fake a gradient output by creating a tensor of ones with the same shape as the output\n",
    "grad_output = torch.ones_like(output_batch)\n",
    "\n",
    "# Backward pass\n",
    "input_batch_grad, kernel_grad = Conv2DFunction.apply(input_batch, kernel).backward(grad_output)\n",
    "\n",
    "print('Input gradient shape:', input_batch_grad.shape)\n",
    "print('Kernel gradient shape:', kernel_grad.shape)\n",
    "print('Input gradient:', input_batch_grad)\n",
    "print('Kernel gradient:', kernel_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End part 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
