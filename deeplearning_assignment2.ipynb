{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Working out backward functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: Let $f(X, Y) = X \\oslash Y$ for two matrices $X$ and $Y$ (where the division is\n",
    "element-wise). Derive the backward for $X$ and for $Y$. Show the derivation.\n",
    "\n",
    "```Answer:```\n",
    "\n",
    "Let $Z = f(X, Y)$. \n",
    "\n",
    "The backward for $X$ is: $ \\frac{\\partial Loss}{\\partial X} = \n",
    "\\frac{\\partial L}{\\partial Z} \\odot \\frac{\\partial Z}{\\partial X}$.\n",
    "\n",
    "Then, we have:\n",
    "\n",
    "$$\n",
    "z_{i,j} = \\frac{x_{i,j}}{y_{i,j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Loss}{\\partial x_{i,j}} = \\sum_{k,l} \\frac{\\partial Loss}{\\partial z_{k,l}} \\cdot \\frac{\\partial z_{k,l}}{\\partial x_{i,j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\sum_{k,l} \\frac{\\partial Loss}{\\partial z_{k,l}} \\cdot \\frac{\\partial \\left (\\frac{x_{k,l}}{y_{k,l}}\\right)}{\\partial x_{i,j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\sum_{k,l} z'_{k,l} \\cdot \\frac{\\partial \\left (\\frac{x_{k,l}}{y_{k,l}}\\right)}{\\partial x_{i,j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\sum_{k,l} z'_{k,l} \\cdot \\frac{1}{y_{i,j}} \\cdot \\delta_{i,k} \\cdot \\delta_{j,l}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = z'_{i,j} \\cdot \\frac{1}{y_{i,j}}\n",
    "$$\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "    X'= Z' \\odot (1 \\oslash Y)\n",
    "$$\n",
    "\n",
    "Similarly, the backward for $Y$ is: $ \\frac{\\partial Loss}{\\partial Y} = \n",
    "\\frac{\\partial L}{\\partial Z} \\odot \\frac{\\partial Z}{\\partial Y}$.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Loss}{\\partial y_{i,j}} = \\sum_{k,l} \\frac{\\partial Loss}{\\partial z_{k,l}} \\cdot \\frac{\\partial z_{k,l}}{\\partial y_{i,j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\sum_{k,l} z'_{i,j} \\cdot \\frac{\\partial \\left (\\frac{x_{k,l}}{y_{k,l}}\\right)}{\\partial y_{i,j}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "    = \\sum_{k,l} z'_{k,l} \\cdot - \\frac{x_{k,l}}{y^2_{k,l}} \\cdot \\delta_{i,k} \\cdot \\delta_{j,l}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "    = z'_{i,j} \\cdot - \\frac{x_{i,j}}{y^2_{i,j}}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "    Y' = Z' \\odot (- X \\oslash Y^2)\n",
    "$$\n",
    "\n",
    "\n",
    "**Question 2**: Let $f$ be a scalar-to-scalar function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. Let $F(X)$ be a tensor-to-tensor\n",
    "function that applies $f$ element-wise (For a concrete example think of the sigmoid function\n",
    "from the lectures). Show that whatever $f$ is, the backward of $F$ is the element-wise\n",
    "application of $f'$ applied to the elements of $X$, multiplied (element-wise) by the gradient of the\n",
    "loss with respect to the outputs.\n",
    "\n",
    "```Answer:```\n",
    "\n",
    "Let $Y = F(X)$, the gradient of the loss with respect to the output is: $\\frac{\\partial Loss}{\\partial Y}$.\n",
    "\n",
    "Then:\n",
    "$$\n",
    "x'_{i,j} = \\frac{\\partial Loss}{x_{i,j}}\n",
    "$$\n",
    "\n",
    "By the chain rule, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial x_{i,j}} = \\frac{\\partial Loss}{\\partial y_{i,j}} \\cdot \\frac{\\partial y_{i,j}}{\\partial x_{i,j}}\n",
    "$$\n",
    "\n",
    "Since $y_{i,j} = f(x_{i,j})$, we have $\\frac{\\partial y_{i,j}}{\\partial x_{i,j}} = f'(x_{i,j})$. Therefore, we can rewrite the equation as:\n",
    "\n",
    "$$\n",
    "x'_{i,j} = \\frac{\\partial Loss}{\\partial y_{i,j}} \\cdot f'(x_{i,j})\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' = \\frac{\\partial Loss}{Y} \\odot f'(X)\n",
    "$$\n",
    "\n",
    "\n",
    "**Question 3**: Let matrix $W$ be the weights of an MLP layer with $f$ input nodes and $m$ output\n",
    "nodes, with no bias and no nonlinearity, and let $X$ be an $n-by-f$ batch of $n$ inputs with $f$\n",
    "features each. Which matrix operation computes the layer outputs? Work out the backward\n",
    "for this operation, providing gradients for both $W$ and $X$\n",
    "\n",
    "```Answer:```\n",
    "\n",
    "\n",
    "Let $Y = X \\cdot W^T$\n",
    "\n",
    "\n",
    "Computing the gradient of the loss with respect to the weights we get: $$\\frac{\\partial Loss}{\\partial w_{ij}} = \\sum_{k=1}^{n} \\frac{\\partial Loss}{\\partial y_{k,i}} \\cdot x_{k,j}$$\n",
    "Computing the gradient of the loss with respect to the inputs we get: $$\\frac{\\partial Loss}{\\partial x_{i,j}} = \\sum_{k=1}^{m} \\frac{\\partial Loss}{\\partial y_{i,k}} \\cdot W_{k,j}$$\n",
    "\n",
    "\n",
    "The result of these operations are: $\\frac{\\partial Loss}{\\partial W} = (\\frac{\\partial Loss}{\\partial Y})^T \\cdot X$,\n",
    "\n",
    "\n",
    "and: $\\frac{\\partial Loss}{\\partial X} = \\frac{\\partial Loss}{\\partial Y} \\cdot W^T$\n",
    "\n",
    "**Question 4**: Let $f(x) = Y$ be a function that takes a vector $\\vec{x}$, and returns the matrix $Y$\n",
    "consisting of 16 columns that are all equal to $\\vec{x}$. Work out the backward of $f$. (This may seem\n",
    "like a contrived example, but it's actually an instance of broadcasting).\n",
    "\n",
    "```Answer:```\n",
    "\n",
    "\n",
    "The derivative of each element $y_{i,j}$ with respect to $x_k$ is given by the Kronecker delta: $$\\frac{\\partial y_{i,j}}{\\partial x_k} = \\delta_{i,k}$$\n",
    "\n",
    "The gradient of the loss with respect to $x_k$ is the sum of the gradients of the loss with respect to the corresponding elements in each column of $Y$: \n",
    "$$\\frac{\\partial Loss}{\\partial x_k} = \\sum_{j=1}^{16} \\frac{\\partial Loss}{\\partial y_{k,j}}$$\n",
    "\n",
    "In matrix form, the gradient with respect to $x$ is the sum of the gradients with respect to each column of $Y$: $$\\frac{\\partial Loss}{\\partial x} = \\sum_{j=1}^{16} \\frac{\\partial Loss}{\\partial y_{j}}$$\n",
    "\n",
    "This means that each element of the gradient with respect to $\\vec{x}$ is equal to the sum of the corresponding elements in the gradient with respect to $Y$. This is a result of the broadcasting operation in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing code from https://github.com/dlvu/vugrad.git\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import vugrad as vg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: Open an `ipython` session or a jupyter notebook in the same directory as the\n",
    "`README.md` file, and import the library with `import vugrad as vg`. Also do `import\n",
    "numpy as np`.\n",
    "\n",
    "Recreate the computation graph `C<-A+B`: create two tensor nodes a and b containing numpy\n",
    "arrays of the same size, and sum them (using the + operator) to create a TensorNode c.\n",
    "Answer the following questions (in words, tell us what these class members mean, don't just\n",
    "copy/paste their values).\n",
    "\n",
    "1) What does `c.value` contain?\n",
    "2) What does `c.source` refer to?\n",
    "3) What does `c.source.inputs[0]`.value refer to?\n",
    "4) What does `a.grad` refer to? What is its current value?\n",
    "<!-- $$\n",
    "\\begin{itemize}\n",
    "    \\item[1] What does c.value contain?\n",
    "    \\item[2] What does c.source refer to?\n",
    "    \\item[3] What does c.source.inputs[0].value refer to?\n",
    "    \\item[4] What does a.grad refer to? What is its current value?\n",
    "\\end{itemize}\n",
    "$$ -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " TensorNode[size (2, 2), source None].\n",
      "B:\n",
      " TensorNode[size (2, 2), source None].\n",
      "C:\n",
      " TensorNode[size (2, 2), source <class 'vugrad.core.Add'>].\n",
      "--------------------------------------------------------------------------------\n",
      "C.value:\n",
      " [[ 0.01862607  1.30675705]\n",
      " [-1.03440556 -0.5177402 ]] \n",
      " C.value contains the value of the tensor C\n",
      "C.source:\n",
      " <vugrad.core.OpNode object at 0x00000293731C5B50> \n",
      " C.source refers to the operation that generates the tensor C\n",
      "C.source.inputs[0].value == A.value:\n",
      " [[ True  True]\n",
      " [ True  True]] \n",
      " C.source.inputs[0].value refers to the value of the tensor A\n",
      "A.grad:\n",
      " [[0. 0.]\n",
      " [0. 0.]] \n",
      " A.grad refers to the gradient of the tensor A, which is currently 0\n"
     ]
    }
   ],
   "source": [
    "# graph node C<-A+B\n",
    "A = vg.TensorNode(np.random.randn(2,2))  # create A tensor\n",
    "B = vg.TensorNode(np.random.randn(2,2))  # create B tensor\n",
    "C = A + B                                # create C tensor\n",
    "\n",
    "print(\"A:\\n\", A)\n",
    "print(\"B:\\n\", B)\n",
    "print(\"C:\\n\", C)\n",
    "print('-'*80)\n",
    "\n",
    "# 1)\n",
    "print(\"C.value:\\n\", C.value, \"\\n\", \n",
    "\"C.value contains the value of the tensor C\")\n",
    "# 2)\n",
    "print(\"C.source:\\n\", C.source, \"\\n\", \n",
    "\"C.source refers to the operation that generates the tensor C\")\n",
    "# 3)\n",
    "print(\"C.source.inputs[0].value == A.value:\\n\", \n",
    "C.source.inputs[0].value == A.value, \"\\n\", \n",
    "\"C.source.inputs[0].value refers to the value of the tensor A\")\n",
    "# 4)\n",
    "print(\"A.grad:\\n\", A.grad, \"\\n\", \n",
    "\"A.grad refers to the gradient of the tensor A, which is currently 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**: You will find the implementation of TensorNode and OpNode in the file\n",
    "vugrad/core.py. Read the code and answer the following questions:\n",
    "1) An OpNode is defined by its inputs, its outputs and the specific operation it represents (i.e.\n",
    "summation, multiplication). What kind of object defines this operation?\n",
    "\n",
    "2) In the computation graph of question 5, we ultimately added one numpy array to another\n",
    "(albeit wrapped in a lot of other code). In which line of code is the actual addition performed?\n",
    "\n",
    "3) When an OpNode is created, its inputs are immediately set, together with a reference to\n",
    "the op that is being computed. The pointer to the output node(s) is left None at first. Why is\n",
    "this? In which line is the OpNode connected to the output nodes?\n",
    "\n",
    "`Answer:`\n",
    "1) The object that defines the operations (i.e.\n",
    "summation, multiplication) is the Op object or class.\n",
    "\n",
    "2) The actual addition is performed on line 324:\n",
    "```python\n",
    "class Add(Op):\n",
    "    \"\"\"\n",
    "    Op for element-wise matrix addition.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(context, a, b):\n",
    "        assert a.shape == b.shape, f'Arrays not the same sizes ({a.shape} {b.shape}).'\n",
    "        return a + b\n",
    "```\n",
    "3) Opnode hasn't compute any value yet, and therefore it does not know in which tensor it has to stores the values.\n",
    "The line is the following:\n",
    "```python\n",
    "outputs = [TensorNode(value=output, source=opnode) for output in outputs_raw]\n",
    "opnode.outputs = outputs\n",
    "```\n",
    "\n",
    "**Question 7**: When we have a complete computation graph, resulting in a TensorNode called\n",
    "loss, containing a single scalar value, we start backpropagation by calling\n",
    "`loss.backward()`\n",
    "Ultimately, this leads to the `backward()` functions of the relevant Ops being called, which do\n",
    "the actual computation. In which line of the code does this happen?\n",
    "NOTE: Don't just give the line numbers (there may be small changes to the code), but show\n",
    "the line in context in your report\n",
    "\n",
    "`answer`\n",
    "\n",
    "It is called at the following snippet, at line 97:\n",
    "\n",
    "```python\n",
    "\n",
    "        # If we've been visited by all parents, move down the tree\n",
    "        if self.visits == self.numparents or start:\n",
    "            if self.source is not None:\n",
    "                self.source.backward()\n",
    "        else:\n",
    "            assert self.visits < self.numparents, f'{self.numparents} {self.visits} {self.name}'\n",
    "```\n",
    "\n",
    "\n",
    "**Question 8**: core.py contains the three main Ops, with some more provided in ops.py.\n",
    "Choose one of the ops Normalize, Expand, Select, Squeeze or Unsqueeze, and show that\n",
    "the implementation is correct. That is, for the given forward, derive the backward by hand\n",
    "(mathematically in LaTeX), and show that it matches what is implemented.\n",
    "\n",
    "The chosen operation is Normalize:\n",
    "\n",
    "The Normalize operation is mathematically defined as:\n",
    "\n",
    "$$\n",
    "y_{i,j} = \\frac{x_{i,j}}{\\sum_{k=1}^n x_{i,k}}\n",
    "$$\n",
    "\n",
    "where $x$ is the input matrix, $y$ is the output matrix, and $n$ is the number of columns.\n",
    "\n",
    "The backward pass is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial x_{i,j}} = \\sum_{k=1}^m \\frac{\\partial Loss}{\\partial y_{k,j}} \\frac{\\partial y_{k,j}}{\\partial x_{i,j}}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of rows. The second term can be further expanded as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{k,j}}{\\partial x_{i,j}} = \\begin{cases}\n",
    "\\frac{1}{\\sum_{k=1}^n x_{i,k}} - \\frac{x_{i,j}}{(\\sum_{k=1}^n x_{i,k})^2} & \\text{if } i = k \\\\\n",
    "- \\frac{x_{k,j}}{(\\sum_{k=1}^n x_{i,k})^2} & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Using these expressions, we put it all together as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial x_{i,j}} = \\frac{\\partial Loss}{\\partial y_{i,j}} \\frac{1}{\\sum_{k=1}^n x_{i,k}} - \\frac{1}{(\\sum_{k=1}^n x_{i,k})^2} \\sum_{k=1}^m \\frac{\\partial Loss}{\\partial y_{k,j}} x_{k,j}\n",
    "$$\n",
    "\n",
    "This matches the implementation in the backward method, where `go` is the gradient of the loss with respect to the output `y`, `x` is the input matrix, and `sumd` is the sum of each row of `x`. The code returns the gradient of the loss with respect to the input `x` as:\n",
    "\n",
    "```python\n",
    "(go / sumd) - ((go * x)/(sumd * sumd)).sum(axis=1, keepdims=True)\n",
    "```\n",
    "**Question 9**: The current network uses a Sigmoid nonlinearity on the hidden layer. Create an\n",
    "Op for a ReLU nonlinearity (details in the last part of the lecture). Retrain the network.\n",
    "Compare the validation accuracy of the Sigmoid and the ReLU versions.\n",
    "\n",
    "```answer```\n",
    "\n",
    "My Relu implementation in ops.py\n",
    "```python\n",
    "class ReLU(Op):\n",
    "    \"\"\"\n",
    "    Op for element-wise application of ReLU function\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(context, input):\n",
    "\n",
    "        context['input'] = input\n",
    "\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, goutput):\n",
    "        input = context['input']\n",
    "\n",
    "        return goutput * (input > 0)\n",
    "```\n",
    "\n",
    "and in functions.py\n",
    "```python\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return ReLU.do_forward(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP\n",
    "# CODE FROM PROFESSORS\n",
    "class MLP(vg.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP with one hidden layer, and a sigmoid non-linearity on the hidden layer and a softmax on the\n",
    "    output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_mult=4, if_relu=False):\n",
    "        \"\"\"\n",
    "        :param input_size:\n",
    "        :param output_size:\n",
    "        :param hidden_mult: Multiplier that indicates how many times bigger the hidden layer is than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = hidden_mult * input_size\n",
    "        # -- There is no common wisdom on how big the hidden size should be, apart from the idea\n",
    "        #    that it should be strictly _bigger_ than the input if at all possible.\n",
    "\n",
    "        self.layer1 = vg.Linear(input_size, hidden_size)\n",
    "        self.layer2 = vg.Linear(hidden_size, output_size)\n",
    "        # -- The linear layer (without activation) is implemented in vugrad. We simply instantiate these modules, and\n",
    "        #    add them to our network.\n",
    "\n",
    "        self.if_relu = if_relu\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        assert len(input.size()) == 2\n",
    "\n",
    "        # first layer\n",
    "        hidden = self.layer1(input)\n",
    "\n",
    "        # non-linearity (sigmoid or relu) added by me\n",
    "        if self.if_relu:\n",
    "            hidden = vg.relu(hidden)\n",
    "        else:\n",
    "            hidden = vg.sigmoid(hidden)\n",
    "        # -- We've called a utility function here, to mimin how this is usually done in pytorch. We could also do:\n",
    "        #    hidden = Sigmoid.do_forward(hidden)\n",
    "\n",
    "        # second layer\n",
    "        output = self.layer2(hidden)\n",
    "\n",
    "        # softmax activation\n",
    "        output = vg.logsoftmax(output)\n",
    "        # -- the logsoftmax computes the _logarithm_ of the probabilities produced by softmax. This makes the computation\n",
    "        #    of the CE loss more stable when the probabilities get close to 0 (remember that the CE loss is the logarithm\n",
    "        #    of these probabilities). It needs to be implemented in a specific way. See the source for details.\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "\n",
    "        return self.layer1.parameters() + self.layer2.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loaded data:\n",
      "         number of instances: 60000 in training, 10000 in validation\n",
      " training class distribution: [32728 27272]\n",
      "     val. class distribution: [5484 4516]\n",
      "## Model with relu: False\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.4459\n",
      "   running loss: 0.524\n",
      "epoch 001\n",
      "       accuracy: 0.9449\n",
      "   running loss: 0.107\n",
      "epoch 002\n",
      "       accuracy: 0.9814\n",
      "   running loss: 0.06469\n",
      "epoch 003\n",
      "       accuracy: 0.987\n",
      "   running loss: 0.05123\n",
      "epoch 004\n",
      "       accuracy: 0.9896\n",
      "   running loss: 0.04456\n",
      "epoch 005\n",
      "       accuracy: 0.9906\n",
      "   running loss: 0.04036\n",
      "epoch 006\n",
      "       accuracy: 0.9914\n",
      "   running loss: 0.03736\n",
      "epoch 007\n",
      "       accuracy: 0.9918\n",
      "   running loss: 0.03506\n",
      "epoch 008\n",
      "       accuracy: 0.9924\n",
      "   running loss: 0.03324\n",
      "epoch 009\n",
      "       accuracy: 0.9926\n",
      "   running loss: 0.03174\n",
      "epoch 010\n",
      "       accuracy: 0.9931\n",
      "   running loss: 0.03046\n",
      "epoch 011\n",
      "       accuracy: 0.9931\n",
      "   running loss: 0.02936\n",
      "epoch 012\n",
      "       accuracy: 0.9932\n",
      "   running loss: 0.02839\n",
      "epoch 013\n",
      "       accuracy: 0.9935\n",
      "   running loss: 0.02753\n",
      "epoch 014\n",
      "       accuracy: 0.9934\n",
      "   running loss: 0.02676\n",
      "epoch 015\n",
      "       accuracy: 0.9936\n",
      "   running loss: 0.02607\n",
      "epoch 016\n",
      "       accuracy: 0.9937\n",
      "   running loss: 0.02543\n",
      "epoch 017\n",
      "       accuracy: 0.9941\n",
      "   running loss: 0.02486\n",
      "epoch 018\n",
      "       accuracy: 0.9942\n",
      "   running loss: 0.02433\n",
      "epoch 019\n",
      "       accuracy: 0.9946\n",
      "   running loss: 0.02384\n",
      "## Model with relu: True\n",
      "\n",
      "## Starting training\n",
      "epoch 000\n",
      "       accuracy: 0.6005\n",
      "   running loss: 0.08556\n",
      "epoch 001\n",
      "       accuracy: 0.9869\n",
      "   running loss: 0.04586\n",
      "epoch 002\n",
      "       accuracy: 0.9859\n",
      "   running loss: 0.04102\n",
      "epoch 003\n",
      "       accuracy: 0.9877\n",
      "   running loss: 0.03795\n",
      "epoch 004\n",
      "       accuracy: 0.9887\n",
      "   running loss: 0.03634\n",
      "epoch 005\n",
      "       accuracy: 0.9898\n",
      "   running loss: 0.0352\n",
      "epoch 006\n",
      "       accuracy: 0.9902\n",
      "   running loss: 0.0337\n",
      "epoch 007\n",
      "       accuracy: 0.9906\n",
      "   running loss: 0.03393\n",
      "epoch 008\n",
      "       accuracy: 0.9901\n",
      "   running loss: 0.03409\n",
      "epoch 009\n",
      "       accuracy: 0.9901\n",
      "   running loss: 0.03385\n",
      "epoch 010\n",
      "       accuracy: 0.9896\n",
      "   running loss: 0.03384\n",
      "epoch 011\n",
      "       accuracy: 0.9893\n",
      "   running loss: 0.03405\n",
      "epoch 012\n",
      "       accuracy: 0.9893\n",
      "   running loss: 0.03396\n",
      "epoch 013\n",
      "       accuracy: 0.9894\n",
      "   running loss: 0.03389\n",
      "epoch 014\n",
      "       accuracy: 0.9894\n",
      "   running loss: 0.0336\n",
      "epoch 015\n",
      "       accuracy: 0.9893\n",
      "   running loss: 0.03351\n",
      "epoch 016\n",
      "       accuracy: 0.9893\n",
      "   running loss: 0.03339\n",
      "epoch 017\n",
      "       accuracy: 0.9892\n",
      "   running loss: 0.03333\n",
      "epoch 018\n",
      "       accuracy: 0.9892\n",
      "   running loss: 0.03304\n",
      "epoch 019\n",
      "       accuracy: 0.9892\n",
      "   running loss: 0.03297\n",
      "## Results:\n",
      "  sigmoid: [0.4459, 0.9449, 0.9814, 0.987, 0.9896, 0.9906, 0.9914, 0.9918, 0.9924, 0.9926, 0.9931, 0.9931, 0.9932, 0.9935, 0.9934, 0.9936, 0.9937, 0.9941, 0.9942, 0.9946] (loss: [0.5240017355543722, 0.10701544484775606, 0.0646865121154042, 0.051234753295661614, 0.04456260241814657, 0.040362992919948805, 0.03735509254434215, 0.035061919702054585, 0.03323815857965607, 0.031735755404077304, 0.030461029258905724, 0.029356629029618345, 0.028387213072088732, 0.027527845076074586, 0.026758932740669882, 0.02606502153890947, 0.025434170414191302, 0.02485721359693162, 0.024327053516476408, 0.023838081219554257])\n",
      "     relu: [0.6005, 0.9869, 0.9859, 0.9877, 0.9887, 0.9898, 0.9902, 0.9906, 0.9901, 0.9901, 0.9896, 0.9893, 0.9893, 0.9894, 0.9894, 0.9893, 0.9893, 0.9892, 0.9892, 0.9892] (loss: [0.08556380983736762, 0.04586121559739964, 0.04102023589132713, 0.037954877365669944, 0.03634203004438603, 0.035204959840076606, 0.03369921143559337, 0.033930637144941586, 0.034088869213699, 0.03384651138600551, 0.033843386956525265, 0.03405363304173182, 0.03396333404140793, 0.03389422648346036, 0.033596765734679956, 0.03351443989561022, 0.03338702748280351, 0.03332557806047433, 0.03303588566579061, 0.03297415616540462])\n"
     ]
    }
   ],
   "source": [
    "# CODE FROM PROFESSORS\n",
    "(xtrain, ytrain), (xval, yval), num_classes = vg.load_synth() # load synthetic data, going easy on my cpu\n",
    "print(f'## loaded data:')\n",
    "print(f'         number of instances: {xtrain.shape[0]} in training, {xval.shape[0]} in validation')\n",
    "print(f' training class distribution: {np.bincount(ytrain)}')\n",
    "print(f'     val. class distribution: {np.bincount(yval)}')\n",
    "num_instances, num_features = xtrain.shape\n",
    "\n",
    "\n",
    "n, m = xtrain.shape\n",
    "b = 128\n",
    "epochs = 20\n",
    "lr = 0.01\n",
    "\n",
    "results = {}\n",
    "\n",
    "if_relus = [False, True]\n",
    "for if_relu in if_relus:\n",
    "    ## Instantiate the model\n",
    "    mlp = MLP(input_size=num_features, \n",
    "              output_size=num_classes,\n",
    "              hidden_mult=4,\n",
    "              if_relu=if_relu)\n",
    "    \n",
    "    print(f'## Model with relu: {if_relu}')\n",
    "    losses, accuracies = [], []\n",
    "    print('\\n## Starting training')\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'epoch {epoch:03}')\n",
    "\n",
    "        ## Compute validation accuracy\n",
    "        o = mlp(vg.TensorNode(xval))\n",
    "        oval = o.value\n",
    "\n",
    "        predictions = np.argmax(oval, axis=1)\n",
    "        num_correct = (predictions == yval).sum()\n",
    "        acc = num_correct / yval.shape[0]\n",
    "\n",
    "        o.clear() # gc the computation graph\n",
    "        print(f'       accuracy: {acc:.4}')\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        cl = 0.0 # running sum of the training loss\n",
    "\n",
    "        # We loop over the data in batches of size `b`\n",
    "        for fr in range(0, n, b):\n",
    "\n",
    "            # The end index of the batch\n",
    "            to = min(fr + b, n)\n",
    "\n",
    "            # Slice out the batch and its corresponding target values\n",
    "            batch, targets = xtrain[fr:to, :], ytrain[fr:to]\n",
    "\n",
    "            # Wrap the inputs in a Node\n",
    "            batch = vg.TensorNode(value=batch)\n",
    "\n",
    "            outputs = mlp(batch)\n",
    "            loss = vg.logceloss(outputs, targets)\n",
    "            # -- The computation graph is now complete. It consists of the MLP, together with the computation of\n",
    "            #    the scalar loss.\n",
    "            # -- The variable `loss` is the TensorNode at the very top of our computation graph. This means we can call\n",
    "            #    it to perform operations on the computation graph, like clearing the gradients, starting the backpropgation\n",
    "            #    and clearing the graph.\n",
    "            # -- Note that we set the MLP up to produce log probabilties, so we should compute the CE loss for these.\n",
    "\n",
    "            cl += loss.value\n",
    "            # -- We must be careful here to extract the _raw_ value for the running loss. What would happen if we kept\n",
    "            #    a running sum using the TensorNode?\n",
    "\n",
    "            # Start the backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # pply gradient descent\n",
    "            for parm in mlp.parameters():\n",
    "                parm.value -= lr * parm.grad\n",
    "                # -- Note that we are directly manipulating the members of the parm TensorNode. This means that for this\n",
    "                #    part, we are not building up a computation graph.\n",
    "\n",
    "            # -- In Pytorch, the gradient descent is abstracted away into an Optimizer. This allows us to build slightly more\n",
    "            #    complexoptimizers than plain graident descent.\n",
    "\n",
    "            # Finally, we need to reset the gradients to zero ...\n",
    "            loss.zero_grad()\n",
    "            # ... and delete the parts of the computation graph we don't need to remember.\n",
    "            loss.clear()\n",
    "\n",
    "        print(f'   running loss: {cl/n:.4}')\n",
    "        losses.append(cl/n)\n",
    "\n",
    "        if if_relu == True: # store results\n",
    "            results['relu'] = accuracies\n",
    "            results['relu_loss'] = losses\n",
    "        else:\n",
    "            results['sigmoid'] = accuracies\n",
    "            results['sigmoid_loss'] = losses\n",
    "\n",
    "print(f'## Results:')\n",
    "print(f'  sigmoid: {results[\"sigmoid\"]} (loss: {results[\"sigmoid_loss\"]})')\n",
    "print(f'     relu: {results[\"relu\"]} (loss: {results[\"relu_loss\"]})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(results['sigmoid'], label='Sigmoid')\n",
    "axes[0].plot(results['relu'], label='ReLu')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "\n",
    "axes[1].plot(results['sigmoid_loss'], label='Sigmoid')\n",
    "axes[1].plot(results['relu_loss'], label='ReLu')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "plt.savefig('sigmoid_relu.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**: Change the network architecture (and other aspects of the model) and show\n",
    "how the training behavior changes for the MNIST data. Here are some ideas (but feel free to\n",
    "try something else).\n",
    "\n",
    " Try adding more layers to the MLP, or widening the network (more nodes in the\n",
    "hidden layer).\n",
    "\n",
    " Add a momentum term to the gradient descent. This is discussed in lecture 4.\n",
    "\n",
    " Try adding a residual connection between layers. These are also discussed in lecture 4.\n",
    "\n",
    " It is often said that good initialization is the key to neural network performance. What\n",
    "happens if you replace the Glorot initialization (used in the Linear module) by\n",
    "something else? If you initialize to all 0s or sample from a standard normal\n",
    "distribution, do you see a drop in performance?\n",
    "\n",
    " If your computer is too slow to do this quickly enough on the MNIST data, you can\n",
    "also work with the synthetic data, but it may be too simple to show the benefit of\n",
    "things like residual connections. In that case, just report what you find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**:\n",
    "Install pytorch using the installation instructions on its main page. Follow the Pytorch\n",
    "60-minute blitz. When you've built a classifier, play around with the hyperparameters\n",
    "(learning rate, batch size, nr of epochs, etc) and see what the best accuracies are that you\n",
    "can achieve. Report your hyperparameters and your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 26 20:01:13 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P8               2W / 105W |      0MiB /  8188MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     21392    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Nov__3_17:51:05_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.103\n",
      "Build cuda_12.3.r12.3/compiler.33492891_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi && nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device_name = torch.cuda.get_device_name()\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12**:\n",
    "Change some (at least one, if large enough of a change. To be safe try three), other aspects\n",
    "of the training and report the results. For instance, the package torch.optim contains other\n",
    "optimizers than SGD which you could try. There are also other loss functions. You could\n",
    "even look into some tricks for improving the network architecture, like batch normalization or\n",
    "residual connections. We haven't discussed these in the lectures yet, but there are plenty of\n",
    "resources available online. Don't worry too much about getting a positive result, just report\n",
    "what you tried and what you found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
